{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3.4 Sentiment Classification using transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains how you can use a transformer model that is fine-tuned for sentiment analysis. Fine-tuned transformer models are published regularly on the huggingface platform: https://huggingface.co/models\n",
    "\n",
    "These models are very big (Gigabytes) and require a computer with sufficient memory to load. Furthermore, loading these models takes some time as well. It is also possible to copy such a model to your disk and to load the local copy. Still a substantial memory is needed to load it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires installing some deep learning packages: transformers, pytorch and simpletransformers. If you are not experienced with installing these packages, make sure you first define a virtual environment for python, activate this environment and install the packages in this enviroment.\n",
    "\n",
    "Please consult the Python documentation for installing such an enviroment:\n",
    "\n",
    "https://docs.python.org/3/library/venv.html\n",
    "\n",
    "After activating your enviroment you can install pytorch, transformers and simpletransformers from the command line. If you start this notebook within the same virtual environment you can also execute the next installation commands from your notebook. Once installed, you can comment out the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.26.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpletransformers\n",
      "  Using cached simpletransformers-0.63.9-py3-none-any.whl (250 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simpletransformers) (1.24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simpletransformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simpletransformers) (4.64.1)\n",
      "Requirement already satisfied: regex in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simpletransformers) (2022.10.31)\n",
      "Requirement already satisfied: transformers>=4.6.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simpletransformers) (4.26.1)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.10.1-cp311-cp311-win_amd64.whl (42.2 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.1-cp311-cp311-win_amd64.whl (8.2 MB)\n",
      "Collecting seqeval\n",
      "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: pandas in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simpletransformers) (1.5.3)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simpletransformers) (0.13.2)\n",
      "Collecting wandb>=0.10.32\n",
      "  Using cached wandb-0.13.10-py3-none-any.whl (2.0 MB)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.19.0-py2.py3-none-any.whl (9.6 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97.tar.gz (524 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: colorama in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.1.3)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Using cached GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "Collecting psutil>=5.0.0\n",
      "  Using cached psutil-5.9.4-cp36-abi3-win_amd64.whl (252 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Using cached sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting pathtools\n",
      "  Using cached pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting setproctitle\n",
      "  Using cached setproctitle-1.3.2-cp311-cp311-win_amd64.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (65.5.0)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0\n",
      "  Using cached protobuf-4.22.0-cp310-abi3-win_amd64.whl (420 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpletransformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpletransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpletransformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpletransformers) (2022.12.7)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Using cached pyarrow-11.0.0-cp311-cp311-win_amd64.whl (20.5 MB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.4-cp311-cp311-win_amd64.whl (317 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->simpletransformers) (2022.7.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->simpletransformers) (1.2.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting altair>=3.2.0\n",
      "  Using cached altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "Collecting blinker>=1.0.0\n",
      "  Using cached blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
      "Collecting cachetools>=4.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting importlib-metadata>=1.4\n",
      "  Using cached importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-9.4.0-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Collecting pympler>=0.9\n",
      "  Using cached Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Using cached rich-13.3.2-py3-none-any.whl (238 kB)\n",
      "Collecting semver\n",
      "  Using cached semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting toml\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit->simpletransformers) (4.5.0)\n",
      "Collecting tzlocal>=1.1\n",
      "  Using cached tzlocal-4.2-py3-none-any.whl (19 kB)\n",
      "Collecting validators>=0.2\n",
      "  Using cached validators-0.20.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pydeck>=0.1.dev5\n",
      "  Using cached pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
      "Collecting tornado>=6.0.3\n",
      "  Using cached tornado-6.2-cp37-abi3-win_amd64.whl (425 kB)\n",
      "Collecting watchdog\n",
      "  Using cached watchdog-2.3.1-py3-none-win_amd64.whl (80 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting grpcio>=1.48.2\n",
      "  Using cached grpcio-1.51.3-cp311-cp311-win_amd64.whl (3.7 MB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard->simpletransformers) (1.8.1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Collecting wheel>=0.26\n",
      "  Using cached wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Collecting entrypoints\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.1.2)\n",
      "Collecting jsonschema>=3.0\n",
      "  Using cached jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Collecting toolz\n",
      "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (22.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.8.2-cp311-cp311-win_amd64.whl (55 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp311-cp311-win_amd64.whl (32 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting markdown-it-py<3.0.0,>=2.2.0\n",
      "  Using cached markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Using cached Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tzdata\n",
      "  Using cached tzdata-2022.7-py2.py3-none-any.whl (340 kB)\n",
      "Collecting decorator>=3.4.0\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\elisa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.2)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Using cached pyrsistent-0.19.3-cp311-cp311-win_amd64.whl (62 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: sentencepiece, pyasn1, pathtools, appdirs, zipp, xxhash, wheel, werkzeug, watchdog, tzdata, tornado, toolz, toml, threadpoolctl, tensorboard-data-server, smmap, setproctitle, sentry-sdk, semver, scipy, rsa, pyrsistent, pympler, pygments, pyasn1-modules, pyarrow, psutil, protobuf, pillow, oauthlib, multidict, mdurl, markdown, grpcio, fsspec, frozenlist, entrypoints, docker-pycreds, dill, decorator, cachetools, blinker, async-timeout, absl-py, yarl, validators, scikit-learn, responses, requests-oauthlib, pytz-deprecation-shim, pydeck, multiprocess, markdown-it-py, jsonschema, importlib-metadata, google-auth, gitdb, aiosignal, tzlocal, seqeval, rich, google-auth-oauthlib, GitPython, altair, aiohttp, wandb, tensorboard, streamlit, datasets, simpletransformers\n",
      "  Running setup.py install for sentencepiece: started\n",
      "  Running setup.py install for sentencepiece: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: sentencepiece is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for sentencepiece did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [23 lines of output]\n",
      "  running install\n",
      "  C:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\sentencepiece\n",
      "  copying src\\sentencepiece/__init__.py -> build\\lib.win-amd64-cpython-311\\sentencepiece\n",
      "  copying src\\sentencepiece/_version.py -> build\\lib.win-amd64-cpython-311\\sentencepiece\n",
      "  copying src\\sentencepiece/sentencepiece_model_pb2.py -> build\\lib.win-amd64-cpython-311\\sentencepiece\n",
      "  copying src\\sentencepiece/sentencepiece_pb2.py -> build\\lib.win-amd64-cpython-311\\sentencepiece\n",
      "  running build_ext\n",
      "  building 'sentencepiece._sentencepiece' extension\n",
      "  creating build\\temp.win-amd64-cpython-311\n",
      "  creating build\\temp.win-amd64-cpython-311\\Release\n",
      "  creating build\\temp.win-amd64-cpython-311\\Release\\src\n",
      "  creating build\\temp.win-amd64-cpython-311\\Release\\src\\sentencepiece\n",
      "  \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.33.31629\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python311\\include -IC:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python311\\Include \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.33.31629\\include\" \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.33.31629\\ATLMFC\\include\" \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\cppwinrt\" /EHsc /Tpsrc/sentencepiece/sentencepiece_wrap.cxx /Fobuild\\temp.win-amd64-cpython-311\\Release\\src/sentencepiece/sentencepiece_wrap.obj /std:c++17 /MT /I..\\build\\root\\include\n",
      "  cl : Command line warning D9025 : overriding '/MD' with '/MT'\n",
      "  sentencepiece_wrap.cxx\n",
      "  src/sentencepiece/sentencepiece_wrap.cxx(2822): fatal error C1083: Cannot open include file: 'sentencepiece_processor.h': No such file or directory\n",
      "  error: command 'C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.33.31629\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "sentencepiece\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install transformers \n",
    "!pip3 install simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface transfomers provides an option to create a **pipeline** to perform a NLP task with a pretrained model: \n",
    "\n",
    "\"The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\"\n",
    "\n",
    "More information can be found here: https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html\n",
    "\n",
    "We will use the pipeline module to load a fine-tuned model to perform senteiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'which' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                Version\n",
      "---------------------- ----------\n",
      "attrs                  22.2.0\n",
      "blis                   0.7.9\n",
      "catalogue              2.0.8\n",
      "certifi                2022.12.7\n",
      "charset-normalizer     3.0.1\n",
      "click                  8.1.3\n",
      "colorama               0.4.6\n",
      "confection             0.0.4\n",
      "cymem                  2.0.7\n",
      "distlib                0.3.6\n",
      "en-core-web-sm         3.5.0\n",
      "et-xmlfile             1.1.0\n",
      "filelock               3.9.0\n",
      "huggingface-hub        0.12.1\n",
      "idna                   3.4\n",
      "Jinja2                 3.1.2\n",
      "joblib                 1.2.0\n",
      "langcodes              3.3.0\n",
      "lxml                   4.9.2\n",
      "MarkupSafe             2.1.2\n",
      "munch                  2.5.0\n",
      "murmurhash             1.0.9\n",
      "nltk                   3.8.1\n",
      "numpy                  1.24.2\n",
      "openpyxl               3.0.10\n",
      "packaging              23.0\n",
      "pandas                 1.5.3\n",
      "pathy                  0.10.1\n",
      "pip                    22.3.1\n",
      "platformdirs           3.0.0\n",
      "plotly                 5.13.0\n",
      "preshed                3.0.8\n",
      "pydantic               1.10.5\n",
      "pyproj                 3.4.1\n",
      "python-dateutil        2.8.2\n",
      "pytz                   2022.7.1\n",
      "PyYAML                 6.0\n",
      "random-username        1.0.2\n",
      "regex                  2022.10.31\n",
      "requests               2.28.2\n",
      "setuptools             65.5.0\n",
      "shapely                2.0.1\n",
      "six                    1.16.0\n",
      "sklearn                0.0.post1\n",
      "smart-open             6.3.0\n",
      "spacy                  3.5.0\n",
      "spacy-legacy           3.0.12\n",
      "spacy-loggers          1.0.4\n",
      "srsly                  2.4.5\n",
      "tenacity               8.1.0\n",
      "tensorboard-plugin-wit 1.8.1\n",
      "thinc                  8.1.7\n",
      "tokenizers             0.13.2\n",
      "tqdm                   4.64.1\n",
      "transformers           4.26.1\n",
      "typer                  0.7.0\n",
      "typing_extensions      4.5.0\n",
      "urllib3                1.26.14\n",
      "vaderSentiment         3.3.2\n",
      "virtualenv             20.19.0\n",
      "wasabi                 1.1.1\n",
      "xlrd                   2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!which pip\n",
    "!pip list\n",
    "!pip uninstall transformers\n",
    "!python -m pip3 install transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a transformer model 'distilbert-base-uncased-finetuned-sst-2-english' that is fine-tuned for binary classification from the Hugging face repository:\n",
    "\n",
    "https://huggingface.co/models\n",
    "\n",
    "We need to load the model for the sequence classifcation and the tokenizer to convert the sentences into tokens according to the vocabulary of the model.\n",
    "\n",
    "Loading the model takes some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5706dbb5e94bdba8b90cd3cb1ce0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636b33b6f5044265b5478b47376c3faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a95ee2fea342469b9913c2b8146aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205dfe9cecfd46dc8368321084eb6ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentimentenglish = pipeline(\"sentiment-analysis\", \n",
    "                            model=\"distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "                            tokenizer=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now created an instantiation of a pipeline that can tokenize any sentence, obtain a sententence embedding from the transformer language model and perform the **sentiment-analysis** task. Let's try it out on an example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pos_en = \"Nice hotel and the service is great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.999881386756897}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentenglish(sentence_pos_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_neg_en = \"The rooms are dirty and the wifi does not work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997870326042175}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentenglish(sentence_neg_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is easy and seems to work very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Dutch fine-tuned transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a fine-tuned Dutch model for Dutch sentiment analysis by creating another pipeline. Again loading this model takes some time. Also note that after loading, both moodels are loaded in memory. So if you have issues loading, you may want to start over and try again just with the Dutch pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9edcb232b44a3ab8a740d63167b68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d34c9a8e454607a0b6258400c2379d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e2d0e0765f4984b7c8f6b1d21519c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f3f3229ebf4e39a74d6a2a86295560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/241k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb1ba1e7b6146dcae5ffbe6b243a6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentimentdutch = pipeline(\"sentiment-analysis\", \n",
    "                          model=\"wietsedv/bert-base-dutch-cased-finetuned-sentiment\", \n",
    "                          tokenizer=\"wietsedv/bert-base-dutch-cased-finetuned-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test it on two similar Dutch sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pos_nl=\"Mooi hotel en de service is geweldig\"\n",
    "sentence_neg_nl=\"De kamers zijn smerig en de wifi doet het niet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'pos', 'score': 0.9999955892562866}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentdutch(sentence_pos_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neg', 'score': 0.6675326824188232}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentdutch(sentence_neg_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work fine too although the score for negative in the second example is much lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting sentence representations using Simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Simpletransformers package is built on top of the transformer package. It simplifies the use of transformers even more and provides excellent documentation: https://simpletransformers.ai\n",
    "\n",
    "The site explains also how you can fine-tune models yourself or even how to build models from scratch, assuming you have the computing power and the data.\n",
    "\n",
    "Here we are going to use it to inspect the sentence representations a bit more. Unfortunately, we need to load the English model again as an instantiation of a RepresentationModel. So if you have memory issues, please stop the kernel and start again from here.\n",
    "\n",
    "Loading the model may gave a lot of warnings. You can ignore these. If you do not have a graphical card (GPU) and or cuda installed to use the GPU you need to set use_cuda to False, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing BertForTextRepresentation: ['distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'pre_classifier.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'classifier.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTextRepresentation were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['encoder.layer.6.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.attention.self.value.bias', 'pooler.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'pooler.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.language_representation import RepresentationModel\n",
    "        \n",
    "#sentences = [\"Example sentence 1\", \"Example sentence 2\"]\n",
    "model = RepresentationModel(\n",
    "        model_type=\"bert\",\n",
    "        model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        use_cuda=False ## If you cannot use a GPU set this to false\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Representationmodel allows you to obtain a sentence encoding. We do that next for the positive English example which consists of 7 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice hotel and the service is great'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pos_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the simpletransformers API the input must be a list even when it is a single sentence. If you pass a string as input, it will turn it into a list of charcaters, each character as a separate sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.encode_sentences([sentence_pos_en], combine_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a numpy array with the shape (1, 9, 768) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 9, 768)\n"
     ]
    }
   ],
   "source": [
    "print(type(word_vectors))\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number indicates the number of sentences, which is **1** in our case. The next digit **9** indicates the number of tokens and the final digit is the number of dimension for each token according to the transformer model, which **768** in case of BERT models.\n",
    "\n",
    "We can ask for the full embedding representation for the first token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of dimensions for the mebdding of the first token: 768\n",
      "[-9.06034887e-01 -4.87977117e-02  4.74802107e-02 -1.13801634e+00\n",
      "  9.35453296e-01 -1.23630416e+00  2.17447853e+00  4.51255798e-01\n",
      "  5.90349019e-01 -1.95086861e+00 -1.47528207e+00 -2.24826545e-01\n",
      "  1.55494559e+00  1.06500769e+00 -1.40686542e-01  7.52872825e-01\n",
      " -1.61901936e-01  7.77709007e-01  5.06455123e-01  1.60853386e+00\n",
      " -1.06702197e+00 -4.81569618e-01  3.72172892e-01  1.15838265e+00\n",
      " -9.21475470e-01 -2.75591284e-01 -4.57448423e-01  8.95897299e-02\n",
      " -5.26742101e-01 -2.69893020e-01 -8.04319620e-01  6.22947991e-01\n",
      "  1.48045027e+00 -1.96724489e-01 -6.02553070e-01 -1.02787495e+00\n",
      " -1.89065874e+00 -1.07499492e+00 -4.31458801e-02  1.59636819e+00\n",
      " -8.58116224e-02  7.70465076e-01 -1.70893526e+00  3.77522223e-02\n",
      "  2.52258629e-01 -6.95083082e-01 -2.25055051e+00 -4.72664148e-01\n",
      " -5.02944648e-01 -3.95256788e-01 -4.64026779e-01  2.86221147e-01\n",
      " -9.66042340e-01  1.08464456e+00 -5.62285364e-01  1.42635727e+00\n",
      " -7.45007455e-01 -7.32180774e-01  1.07793629e+00  2.93841839e-01\n",
      " -6.53305590e-01 -2.99712038e+00 -4.29297954e-01 -1.70587969e+00\n",
      " -8.89784321e-02  1.45969594e+00  4.18682724e-01 -1.41528457e-01\n",
      " -7.37636864e-01  3.88806701e-01 -3.84465568e-02 -1.15982151e+00\n",
      " -4.47523028e-01  1.91834831e+00 -8.18558991e-01 -1.99417043e+00\n",
      "  7.54048884e-01  8.52511048e-01 -9.64412019e-02  9.60579216e-01\n",
      " -5.42208970e-01 -3.64845932e-01  1.44241929e+00 -3.58821869e-01\n",
      " -5.11861920e-01 -2.86352307e-01  7.22278714e-01 -7.12089598e-01\n",
      "  9.23050582e-01  1.47935167e-01  1.81738734e+00  7.62057126e-01\n",
      "  3.95553708e-01  2.71743327e-01  6.10158503e-01 -2.61577755e-01\n",
      " -1.35114551e+00 -4.62989867e-01  8.40545893e-02  3.40152651e-01\n",
      "  3.18820500e+00  1.02611816e+00 -1.96607316e+00  3.00038278e-01\n",
      " -3.39033343e-02 -8.09343696e-01  4.70785052e-01  2.37757228e-02\n",
      " -1.34010181e-01  2.67901421e-01  3.54188345e-02 -8.62564683e-01\n",
      "  8.95021111e-02 -8.94314110e-01 -1.87260020e+00  5.49440861e-01\n",
      " -3.76265496e-01 -1.45566440e+00 -7.14599788e-01 -1.49126661e+00\n",
      "  8.25139344e-01  2.07454368e-01 -1.23577595e-01 -7.74466991e-01\n",
      " -2.49903351e-01 -3.04470897e-01  8.44945848e-01  5.42596757e-01\n",
      " -1.57812166e+00  1.47972107e+00 -5.08337200e-01 -1.54630947e+00\n",
      " -7.77113795e-01  1.42729282e-01  5.67804813e-01 -1.71235546e-01\n",
      " -1.04236491e-01 -2.67008871e-01 -7.45455384e-01 -1.11264062e+00\n",
      "  9.60265815e-01 -1.27637494e+00 -4.97361988e-01 -5.25569379e-01\n",
      " -8.50579083e-01 -3.49265158e-01 -1.41564488e+00 -8.88748288e-01\n",
      "  2.65358657e-01  3.50338221e-01 -9.77516770e-02 -7.14224994e-01\n",
      "  5.35917580e-01 -3.38195205e-01 -8.66486251e-01  1.18489442e-02\n",
      " -8.70187700e-01  2.39516273e-01  7.87520051e-01 -1.64936030e+00\n",
      "  1.03958035e+00  1.90307453e-01 -7.87380576e-01 -9.40666676e-01\n",
      "  3.63044530e-01  7.11079299e-01 -7.06126034e-01 -2.69994378e-01\n",
      "  7.88414001e-01 -9.09842700e-02  1.28902256e+00  1.14921868e+00\n",
      " -3.30400229e-01 -1.45140779e+00 -1.13534391e+00  5.80567658e-01\n",
      " -2.43177101e-01  1.37497497e+00  8.74472976e-01  1.03506017e+00\n",
      "  7.43156374e-01 -1.52092803e+00 -1.45975471e+00 -2.97296673e-01\n",
      "  4.51909564e-03 -4.34980124e-01  7.53351927e-01  1.04937434e+00\n",
      " -2.05273795e+00 -8.94968629e-01 -4.29359496e-01  2.36008808e-01\n",
      " -3.38047743e-01  2.91884720e-01  5.18366396e-01  1.17010033e+00\n",
      "  1.26152828e-01  1.33005178e+00  5.08046746e-01 -9.93044913e-01\n",
      "  1.13516724e+00  1.00999057e-01  1.14351928e+00 -6.04529269e-02\n",
      "  3.24680895e-01  7.98149526e-01  9.39324081e-01 -1.67277262e-01\n",
      "  6.83429778e-01 -2.83685064e+00  5.56346059e-01 -3.74757618e-01\n",
      "  9.66521502e-01  2.29278302e+00 -1.63830698e+00 -1.82361197e+00\n",
      "  1.07514012e+00  1.70868069e-01  6.23601794e-01  4.35412556e-01\n",
      "  2.33193564e+00  2.67747678e-02 -7.06861377e-01 -4.03538227e-01\n",
      " -7.88596392e-01  2.28738457e-01  9.63154137e-01  6.70415699e-01\n",
      "  9.44243550e-01  1.44322145e+00  5.20532846e-01  9.81102109e-01\n",
      " -7.06681490e-01 -1.44413066e+00 -5.32305419e-01  8.52286398e-01\n",
      "  4.60170239e-01  1.84437305e-01  9.17877793e-01 -1.71476796e-01\n",
      "  1.19304076e-01  1.67134154e+00 -4.65282351e-01  7.17387795e-01\n",
      "  8.92807484e-01  4.23676610e-01 -6.68127954e-01 -5.72310798e-02\n",
      "  8.31583202e-01 -8.69424760e-01  1.23160124e-01  4.82094646e-01\n",
      "  1.23009765e+00 -1.04581106e+00 -1.12619781e+00  3.18066180e-01\n",
      "  1.24819958e+00 -1.12468965e-01 -1.48657277e-01  3.23588461e-01\n",
      " -4.78058577e-01  1.43155611e+00 -8.86060357e-01  1.62254500e+00\n",
      " -2.87217855e+00 -3.98188025e-01  2.33030990e-01  1.16395462e+00\n",
      "  1.44445014e+00 -5.14135122e-01 -6.70574367e-01  2.77586311e-01\n",
      " -1.23880446e+00  1.54260814e+00  4.28140134e-01  2.62978959e+00\n",
      "  5.81721961e-03 -2.82474220e-01  5.94109595e-01  1.34973288e+00\n",
      "  2.21382812e-01  1.45247638e+00 -2.70515054e-01 -1.53290045e+00\n",
      " -1.33196533e+00  4.57388908e-02  1.68617916e+00  1.06477118e+00\n",
      " -1.71973463e-02  1.08750629e+00 -7.20071495e-02  1.03867996e+00\n",
      " -9.72635686e-01 -9.52837706e-01  6.99094236e-01 -3.53749365e-01\n",
      "  1.14603853e+00  6.58969998e-01  2.35470057e-01  3.22577715e-01\n",
      "  1.26276767e+00 -5.40706396e-01 -2.74085045e-01  3.77773941e-01\n",
      " -4.53863412e-01 -4.70761776e-01  7.21004307e-01 -1.48245990e+00\n",
      "  7.10051298e-01 -1.13510966e+00  2.37526730e-01  6.41953051e-01\n",
      " -1.30464017e+00  7.43300557e-01  4.90656763e-01 -7.41154492e-01\n",
      "  2.69123721e+00 -2.31738043e+00 -5.85254788e-01  4.89352912e-01\n",
      " -3.47633176e-02  1.69457301e-01  5.94325483e-01 -3.44057918e-01\n",
      " -1.05651534e+00 -8.19096193e-02  8.41559529e-01  3.49203110e-01\n",
      " -3.57682586e-01 -1.46633768e+00  6.21186197e-01 -1.82282948e+00\n",
      "  1.04514921e+00  3.07525516e-01  5.18729031e-01 -4.05524015e-01\n",
      "  5.07953614e-02 -1.11463404e+00  4.88861799e-01  9.08445477e-01\n",
      " -7.21119523e-01  6.33560836e-01  2.49970585e-01 -5.09224892e-01\n",
      "  8.01383674e-01  1.55633903e+00 -1.17471254e+00  1.01793230e-01\n",
      "  5.86333334e-01  2.39796221e-01 -6.25740588e-01  3.31388474e-01\n",
      "  3.92334908e-01  1.12040961e+00 -1.22438514e+00  2.54766941e-01\n",
      " -5.65961540e-01  1.35146752e-01 -1.00368261e+00 -1.19822276e+00\n",
      " -1.55367732e+00 -1.23215508e+00  5.26886761e-01  7.52496600e-01\n",
      "  1.06138289e+00  7.23407626e-01 -2.09435892e+00 -1.02461003e-01\n",
      " -9.12182987e-01  4.30218399e-01 -4.64545675e-02  1.31438100e+00\n",
      "  8.41391981e-01  3.16828012e-01  1.74729490e+00  2.57660653e-02\n",
      " -4.40375328e-01 -2.35306963e-01 -4.36674058e-01 -1.00714326e+00\n",
      "  1.10797203e+00  1.64410985e+00  1.04372537e+00 -1.11738265e+00\n",
      "  1.77037084e+00 -7.01005578e-01 -8.76743257e-01 -4.71625447e-01\n",
      "  4.04763341e-01  2.25310430e-01  9.09966946e-01  6.61559820e-01\n",
      "  5.38316727e-01 -3.59120816e-01 -1.30571103e+00  7.61048496e-01\n",
      "  7.70875951e-04  2.48299539e-01 -2.92902105e-02 -4.09149766e-01\n",
      "  1.22324789e+00 -1.50463116e+00  9.57270801e-01  8.63787174e-01\n",
      "  4.88637298e-01  7.89198875e-01 -1.01938076e-01  3.64931047e-01\n",
      " -7.60453820e-01 -1.41451645e+00  5.17366469e-01  8.01575840e-01\n",
      "  1.59485650e+00 -8.90860140e-01 -8.01917315e-01 -1.04335809e+00\n",
      " -5.62450230e-01  4.78140935e-02  3.81619573e-01  4.28940952e-01\n",
      " -8.92492414e-01 -8.23444486e-01  1.15371358e+00 -6.83891535e-01\n",
      "  1.88049048e-01 -2.13048673e+00  2.16055107e+00 -1.39106262e+00\n",
      "  1.76622897e-01  5.21972120e-01 -4.82339412e-01 -3.81550848e-01\n",
      "  2.07070142e-01  9.08594429e-01  2.56061524e-01  2.29597613e-02\n",
      " -5.00237465e-01  3.04376721e-01  1.98113668e+00  2.10468695e-01\n",
      " -9.84253287e-01  5.44103324e-01 -1.59976065e+00 -1.77722681e+00\n",
      "  2.35313058e+00 -7.34766662e-01  1.45260409e-01  1.94984281e+00\n",
      " -4.68137205e-01  6.57310244e-03  2.74452120e-01 -1.03841734e+00\n",
      "  5.10510862e-01 -1.40159738e+00  6.27748489e-01 -6.02760136e-01\n",
      "  1.46047875e-01 -2.06167221e-01  1.22674286e+00  4.29852098e-01\n",
      "  9.10036862e-01 -5.42319775e-01 -6.61489069e-02  9.23222527e-02\n",
      " -2.05434537e+00  4.41631973e-02  7.92933464e-01 -4.18377191e-01\n",
      " -1.40826571e+00 -2.27146685e-01  8.18786383e-01 -3.09065670e-01\n",
      " -8.84685278e-01 -1.19191563e+00 -2.12372363e-01  3.57988238e-01\n",
      "  1.51447535e-01  3.97478312e-01  2.36687556e-01 -1.36952007e+00\n",
      "  1.85491526e+00  6.50866807e-01 -2.57349432e-01 -1.40167363e-02\n",
      "  5.56785762e-01  5.12057364e-01  1.18453212e-01  1.10424495e+00\n",
      "  1.54707217e+00 -1.76472664e-01  3.97526383e-01 -2.13138890e+00\n",
      " -9.67942327e-02  8.81871641e-01 -2.27780417e-01 -5.49767137e-01\n",
      " -1.47384942e+00 -5.96329607e-02  6.86559379e-01  2.28829241e+00\n",
      " -4.17023450e-01 -1.16802251e+00 -8.38122010e-01  1.31451762e+00\n",
      " -1.13141036e+00 -7.11570084e-01 -1.50004077e+00  5.75625062e-01\n",
      " -1.50633562e+00  3.89733881e-01 -3.32739919e-01 -1.90660810e+00\n",
      " -1.37514651e+00 -8.01189318e-02 -4.76604551e-01 -1.32887685e+00\n",
      "  9.33761537e-01  4.74698454e-01 -1.63079691e+00  2.31911376e-01\n",
      " -7.24081457e-01  4.02946919e-01  8.41045856e-01  6.53318107e-01\n",
      "  1.12668931e+00  3.18622440e-01  1.79346632e-02  1.72186816e+00\n",
      " -1.40720034e+00 -1.67943609e+00 -2.20917925e-01  1.19568288e-01\n",
      "  1.02345681e+00  7.56434202e-01 -4.22424078e-01  5.78628600e-01\n",
      "  7.39853799e-01  1.08019221e+00 -1.32542706e+00  1.83736646e+00\n",
      " -5.46652317e-01  4.72940624e-01  1.21681178e+00  1.18047738e+00\n",
      "  5.87131739e-01 -1.21749270e+00  1.09672773e+00 -1.13381052e+00\n",
      "  5.34172773e-01 -6.07124507e-01  7.56956935e-01 -1.06962550e+00\n",
      "  9.96519268e-01 -1.09647714e-01 -1.28292644e+00 -1.38294184e+00\n",
      " -1.54067934e-01 -7.08791465e-02 -5.49362361e-01 -7.69001603e-01\n",
      "  8.03669512e-01 -2.10303283e+00 -7.01989472e-01 -3.57691705e-01\n",
      " -6.91249251e-01  3.18692660e+00 -9.34318483e-01 -2.46346608e-01\n",
      "  1.53263664e+00  1.12827885e+00  5.68637609e-01 -1.07138538e+00\n",
      "  2.11463004e-01  2.60071345e-02  7.57137179e-01  8.37234929e-02\n",
      " -2.32214594e+00 -9.14035976e-01 -1.84386981e+00 -1.09008312e+00\n",
      "  2.14879942e+00  1.64098442e+00 -8.49454045e-01 -1.69535995e-01\n",
      " -2.01595187e+00  1.49881423e+00  1.10718286e+00  8.34038794e-01\n",
      " -7.66058713e-02  7.33857676e-02 -2.06120586e+00  6.39505684e-01\n",
      " -4.99718994e-01  1.36726820e+00 -6.47442222e-01  4.77229416e-01\n",
      "  5.61690569e-01 -1.07773080e-01  2.40975618e+00 -3.40935737e-02\n",
      " -3.38175118e-01 -7.46026874e-01  1.28536499e+00 -1.96954405e+00\n",
      "  3.03345889e-01  4.57915813e-01  1.74013615e-01 -1.93666494e+00\n",
      " -1.76634595e-01  2.65367832e-02  4.89079982e-01  1.63411784e+00\n",
      "  3.76026690e-01 -3.09762478e-01 -5.34050882e-01 -5.90050161e-01\n",
      " -1.96364629e+00 -4.94391054e-01  5.24093390e-01 -1.24175465e+00\n",
      "  1.18793547e-02  2.94328272e-01 -8.29813361e-01  8.33415270e-01\n",
      "  8.51017058e-01  1.38969433e+00  7.24436164e-01 -7.95376122e-01\n",
      " -1.22885716e+00 -1.58486092e+00  1.61974478e+00  3.32112879e-01\n",
      "  9.60521400e-02 -5.52183390e-01 -1.12014495e-01 -3.57163340e-01\n",
      " -4.16329950e-01  8.76720846e-02  1.57773399e+00  2.47125888e+00\n",
      " -5.28225303e-01  1.03959739e+00 -1.99422467e+00  3.63838613e-01\n",
      " -6.57889009e-01 -1.59046337e-01 -3.48141968e-01 -1.15848589e+00\n",
      "  1.59112942e+00  1.16150033e+00 -3.40252519e-01 -6.08658075e-01\n",
      " -8.95557642e-01  1.54178274e+00 -4.05195922e-01  1.61587453e+00\n",
      " -4.71784979e-01 -1.31200874e+00  1.71059716e+00  3.32267024e-02\n",
      "  1.33606466e-02 -4.72232312e-01 -7.30192304e-01  1.16649067e+00\n",
      "  8.21994066e-01 -1.48046017e+00 -1.02225944e-01 -3.62124860e-01\n",
      "  1.02539980e+00  5.41996479e-01 -2.47097760e-02 -1.53121626e+00\n",
      "  2.69298285e-01  2.67334580e-01  4.20385450e-01  1.56565559e+00\n",
      "  8.94566923e-02 -3.78206134e-01 -1.06108713e+00  8.59677255e-01\n",
      " -1.37187946e+00  1.50781500e+00  4.47342575e-01  2.29801819e-01\n",
      "  7.22960472e-01  5.88081837e-01  3.06887478e-01 -1.31809962e+00\n",
      " -1.24003005e+00  1.51298869e+00  1.03002593e-01 -7.91961610e-01\n",
      "  9.74670470e-01 -6.04503095e-01  1.34321022e+00 -3.76953816e+00\n",
      "  1.06534684e+00 -6.23296201e-01 -7.02037394e-01  2.06546649e-01\n",
      "  4.94997174e-01  6.11132622e-01  9.10901666e-01 -8.75079989e-01\n",
      "  7.69246340e-01  2.90594641e-02 -6.12848401e-01 -4.19372946e-01\n",
      "  8.81203890e-01  3.78705919e-01 -6.71203792e-01  2.64945126e+00\n",
      " -9.15809691e-01 -6.93110049e-01 -2.38190204e-01 -7.78430045e-01\n",
      " -3.57236773e-01 -1.10805345e+00 -1.50096107e+00 -1.25460970e+00\n",
      "  4.15531397e-01  4.03152227e-01 -1.38640058e+00  3.59243870e-01\n",
      "  4.63782012e-01  2.20250279e-01 -1.40001798e+00 -3.94667178e-01\n",
      " -6.94798350e-01  9.69558895e-01  4.47326571e-01  7.01332092e-01\n",
      " -1.26222527e+00 -4.14846927e-01 -1.09172881e+00 -6.54620111e-01\n",
      "  3.91920596e-01 -1.40331483e+00  6.52924716e-01  1.18211865e-01\n",
      " -1.41226518e+00  1.93828726e+00  4.77248013e-01  4.18662317e-02\n",
      "  1.74362287e-01  1.71199238e+00 -4.25954282e-01  1.34365827e-01\n",
      "  5.94288528e-01 -6.75051153e-01 -6.12145662e-01 -5.37261605e-01\n",
      " -1.49712622e-01  2.49414945e+00  3.56180072e+00 -8.71541947e-02\n",
      "  1.74360037e+00 -3.00961416e-02  1.41981936e+00 -8.51334214e-01\n",
      "  4.92876887e-01  7.34184682e-01 -7.53622949e-01 -7.01130629e-01\n",
      " -1.55151415e+00  9.15994167e-01 -1.47988915e+00 -1.28032768e+00]\n"
     ]
    }
   ],
   "source": [
    "print('Nr of dimensions for the mebdding of the first token:', len(word_vectors[0][0]))\n",
    "print(word_vectors[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WAIT** Our sentence has 7 words so why do we get 9 tokens here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can  use the tokenizer of the model to get the token representation of the transformer and check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 100, 3309, 1998, 1996, 2326, 2003, 2307, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = model.tokenizer(sentence_pos_en)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our sentence has 7 words, we get 9 identifiers. We can use the **decode** function to convert them back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ C L S ]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first token is the special token **CLS** which is an abstract sentence representation. Let's check another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h o t e l'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(3309)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allright, this a word from our sentence. Let's decode them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 [ C L S ]\n",
      "100 [ U N K ]\n",
      "3309 h o t e l\n",
      "1998 a n d\n",
      "1996 t h e\n",
      "2326 s e r v i c e\n",
      "2003 i s\n",
      "2307 g r e a t\n",
      "102 [ S E P ]\n"
     ]
    }
   ],
   "source": [
    "tokenid_list = tokenized_sentence['input_ids']\n",
    "for token_id in tokenid_list:\n",
    "    print(token_id, model.tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer model added the special tokens **CLS** and **SEP** but also represented our \"Nice\" with the **UNK** token. Any idea why? Check the name of the model we used....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the uncased model, which means that for training all inoput was downcased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextMining",
   "language": "python",
   "name": "textmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
