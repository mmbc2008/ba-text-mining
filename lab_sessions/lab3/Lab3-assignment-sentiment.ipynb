{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 ANSWER:\n",
    "\n",
    "‚Ä¢ Sentence 1: The sentence \"I love apples\", is mostly classified as positive (0.808), this is to be expected and due to the positive sentiment rating of the word love in the lexicon used by VADER.\n",
    "‚Ä¢ Sentence 2: The sentence \"I don't love apples\" is classified as negative. This is reasonable since it's the negation of the previous sentence (the same sentence but including \"don't\")\n",
    "‚Ä¢ Sentence 3: The sentence \"I love apples :-)\" is classified as even more positive than the first sentence. This is due to the smiley emoticon, which is also included in the lexicon with positive sentiment rating.\n",
    "‚Ä¢ Sentence 4: \"These houses are ruins\" is classified between neutral and negative. This is also reasonable. Ruins has a negative sentiment rating but not as low as other words. According to this context, it could however make sense if the sentence was classified as more negative than it did.\n",
    "‚Ä¢ Sentence 5: \"These houses are certainly not considered ruins\" has a similar value for the neutral and positive sentiment ratings (around .5), this is also reasonable since it's the negation of the previous sentence, that was classified between neutral and negative.\n",
    "‚Ä¢ Sentence 6: \"He lies in the chair in the garden\". This sentence is classified as neutral, however it's also partially negative, which is not fitting but it's probably due to the word \"lies\" having several meanings, some of which are negative.\n",
    "‚Ä¢ Sentence 7: \"This house is like any house\". This sentence is mostly neutral which makes sense. Again, here there is a word with several meanings (\"like\"), which is skewing the results making them more positive than they otherwise would be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'positive', 'text_of_tweet': '‚ÄúcHiNa cAn‚ÄôT iNnOvAtE.‚Äù üí•Analysis by ASPI* shows that China leads the USA in whopping 37 out of 44 critical scientific areas such as AI, quantum computing, biotech, and advanced materials.<br><br>*funded by U.S. military industrial complex, so no pro-China bias <a href=\"https://t.co/CgNUmGA0iE\"> pic.twitter.com/CgNUmGA0iE', 'tweet_url': 'https://twitter.com/Kanthan2030/status/1631622840989675520?ref_src=twsrc%5Etfw'}\n",
      "2 {'sentiment_label': 'negative', 'text_of_tweet': 'AMERICAN WAR MACHINE NOW FOCUSED ON CHINA<br> <a href=\"https://t.co/5zUMGxoXNQ\">pic.twitter.com/5zUMGxoXNQ</a></p>&mdash; The_Real_Fly (@The_Real_Fly)', 'tweet_url': 'https://twitter.com/The_Real_Fly/status/1631542150675529729?ref_src=twsrc%5Etfw'}\n",
      "3 {'sentiment_label': 'negative', 'text_of_tweet': 'China appears to be requiring foreign law professors to submit their syllabuses to ensure they are following a doctrine pushed by President Xi Jinping <a href=\"https://t.co/SuSWhELiCx\">https://t.co/SuSWhELiCx</a></p>&mdash; Bloomberg (@business)', 'tweet_url': 'https://twitter.com/business/status/1631576391954169857?ref_src=twsrc%5Etfw'}\n",
      "4 {'sentiment_label': 'negative', 'text_of_tweet': 'The United States has added two subsidiaries of Chinese genetics company BGI to a trade blacklist over allegations it conducted genetic analysis and surveillance activities for Beijing, which Washington says was used to repress ethnic minorities in China <a href=\"https://t.co/siXR57whNs\">https://t.co/siXR57whNs</a></p>&mdash; CNN (@CNN)', 'tweet_url': 'https://twitter.com/CNN/status/1631622994924544001?ref_src=twsrc%5Etfw'}\n",
      "5 {'sentiment_label': 'positive', 'text_of_tweet': 'China has a prevalent weapon magazine culture which I can‚Äôt find in America. There are about 2 dozens of highly professional monthlies published and penned by the MIC itself covering every branch of the armed forces. You can buy these magazines at every street corner across the <a href=\"https://t.co/YVNteeP3Iq\">pic.twitter.com/YVNteeP3Iq</a></p>&mdash; Governor General (@manchuxi)', 'tweet_url': 'https://twitter.com/manchuxi/status/1631534583475830788?ref_src=twsrc%5Etfw'}\n",
      "6 {'sentiment_label': 'negative', 'text_of_tweet': 'China is building six times more new coal plants than the rest of the world combined, new research shows <a href=\"https://t.co/zd7akk1eqV\">https://t.co/zd7akk1eqV</a></p>&mdash; ABC News (@abcnews)', 'tweet_url': 'https://twitter.com/abcnews/status/1631450164375478272?ref_src=twsrc%5Etfw'}\n",
      "7 {'sentiment_label': 'negative', 'text_of_tweet': 'China\\'\\'s turn towards fascism is accelerating <a href=\"https://t.co/Bpoey4WnAz\">pic.twitter.com/Bpoey4WnAz</a></p>&mdash; Chinese History Expert (@chineseciv)', 'tweet_url': 'https://twitter.com/chineseciv/status/1631515516207788033?ref_src=twsrc%5Etfw'}\n",
      "8 {'sentiment_label': 'positive', 'text_of_tweet': 'China has a &quot;stunning lead&quot; in 37 out of 44 critical and emerging technologies as Western democracies lose a global competition for research output, a security think tank said on Thursday after tracking defense, space, energy and biotechnology. <a href=\"https://t.co/icY1FHvVGK\">https://t.co/icY1FHvVGK</a></p>&mdash; NEWSMAX (@NEWSMAX)', 'tweet_url': 'https://twitter.com/NEWSMAX/status/1631523549122007040?ref_src=twsrc%5Etfw'}\n",
      "9 {'sentiment_label': 'negative', 'text_of_tweet': \"I'm just wondering if there is any person in Taiwan who thinks that the Biden neocons are pumping billions of dollars of weapons onto their Island and antagonizing China to make them safer?</p>&mdash; Garland Nixon (@GarlandNixon)\", 'tweet_url': 'https://twitter.com/GarlandNixon/status/1631451970752978947?ref_src=twsrc%5Etfw'}\n",
      "10 {'sentiment_label': 'negative', 'text_of_tweet': 'In response to US actions, China will take retaliatory measures to protect Chinese corporations ‚Äî Ministry of Commerce of the People&#39;s Republic of China</p>&mdash; AZ üõ∞üåèüåçüåé (@AZgeopolitics)', 'tweet_url': 'https://twitter.com/AZgeopolitics/status/1631653133104345088?ref_src=twsrc%5Etfw'}\n",
      "11 {'sentiment_label': 'negative', 'text_of_tweet': 'Today is March 3, 2023 and Joe Biden is still an illegitimate President and is owned by China!</p>&mdash; PISSED OFF PATRIOT HOFFY üñï (@PATRIOTGHOFFY)', 'tweet_url': 'https://twitter.com/PATRIOTGHOFFY/status/1631645105684635648?ref_src=twsrc%5Etfw'}\n",
      "12 {'sentiment_label': 'negative', 'text_of_tweet': 'Let me ask you, how long would a China Police Station last in the US, Great Britain, Australia, Japan France, New Zealand. And you know if there was a threat of election interference this would be investigated even before the public demand them to do so. ü§îüá®üá≥ is so inbedded', 'tweet_url': 'https://t.co/Lfxx4UD0wg'}\n",
      "13 {'sentiment_label': 'negative', 'text_of_tweet': 'Wicked cleverness: China wages border aggression against India and then repeatedly advises India to not let the border situation come in the way of bilateral cooperation. China&#39;s latest statement says India should put the border issue in &quot;the proper place in bilateral relations.‚Äù</p>&mdash; Brahma Chellaney (@Chellaney)', 'tweet_url': 'https://twitter.com/Chellaney/status/1631610600781647872?ref_src=twsrc%5Etfw'}\n",
      "14 {'sentiment_label': 'negative', 'text_of_tweet': \"It's fascinating that our gov&#39;t suddenly admits all the facts about COVID&#39;s origin, now that China has decided to side with Russia.</p>&mdash; Shukri Abdirahman (@ShuForCongress)\", 'tweet_url': 'https://twitter.com/ShuForCongress/status/1631653770147889153?ref_src=twsrc%5Etfw'}\n",
      "15 {'sentiment_label': 'negative', 'text_of_tweet': 'The public is inching closer and closer to the harsh reality.<br>Russia and China‚Äôs displeasure with US biological activity in Ukraine, is because of Covid. <br>Western Criminals created SARS-CoV-2, which killed millions of people, and now the Eastern world is angry.</p>&mdash; D-Bark (@DBark46107258)', 'tweet_url': 'https://twitter.com/DBark46107258/status/1631650236279173120?ref_src=twsrc%5Etfw'}\n",
      "16 {'sentiment_label': 'negative', 'text_of_tweet': 'Folks, China got what they wanted from Harper. That 31-year trade deal. And they got to execute Canadians.<br><br>Trudeau is less biddable.<br><br>China wants the CPC back in office, so they&#39;ve set this up. <br><br>That&#39;s what&#39;s going on here, IMO.<a href=\"https://twitter.com/hashtag/cdnpoli?src=hash&amp;ref_src=twsrc%5Etfw\">#cdnpoli</a></p>&mdash; Timothy Anderson üíâüíâüíâüíâüíâüé∂ (@AndersonBooz)', 'tweet_url': 'https://twitter.com/AndersonBooz/status/1631545345556779009?ref_src=twsrc%5Etfw'}\n",
      "17 {'sentiment_label': 'negative', 'text_of_tweet': 'Blinken‚Äô trip to Uzbekistan has only one purpose‚Ä¶ to sow the seeds of regime change that would allow the U.S. Empire to take control of the country in a few years time and turn it into a dagger on the side of China &amp; Russia.</p>&mdash; ÂÄ™ÊòéËææ (Ni Mingda) (@NiMingda_GG)', 'tweet_url': 'https://twitter.com/NiMingda_GG/status/1631642321933484034?ref_src=twsrc%5Etfw'}\n",
      "18 {'sentiment_label': 'negative', 'text_of_tweet': 'There is ten times more evidence of Biden-China collusion than there ever was of Trump-Russia collusion.<br><br>The Hunter Biden laptop is a smoking gun.<br><br>When have the lamestream media brought this up? Where&#39;s the campaign surveillance? When&#39;s a Special Counsel going to investigate?</p>&mdash; Kyle Becker (@kylenabecker)', 'tweet_url': 'https://twitter.com/kylenabecker/status/1631654725367021569?ref_src=twsrc%5Etfw'}\n",
      "19 {'sentiment_label': 'negative', 'text_of_tweet': 'üá®üá≥üá∫üá∏: The heat is turning up <br><br>&quot;We strongly oppose the sale of arms to Chinese Taiwan...<br>We demand that the US cease arms sales to Taiwan and cease military ties with the island.&quot; <br>The People&#39;s Liberation Army of China is always ready to strike back...&quot;<br>-spokesman Tan Kefei<br>--&gt;üëá</p>&mdash; David Roth-Lindberg (@RothLindberg)', 'tweet_url': 'https://twitter.com/RothLindberg/status/1631635667154337794?ref_src=twsrc%5Etfw'}\n",
      "20 {'sentiment_label': 'negative', 'text_of_tweet': 'A report from the Australian Institute for Strategic Policy Research warns that China is achieving a significant advantage over the US and the West in the vast majority of critical and advanced technologies.<br><br>According to the report, China leads in 37 out of 44 technologies‚Ä¶ <a href=\"https://t.co/namahAiBT2\">https://t.co/namahAiBT2</a></p>&mdash; GraphicW (@GraphicW5)', 'tweet_url': 'https://twitter.com/GraphicW5/status/1631634185742868480?ref_src=twsrc%5Etfw'}\n",
      "21 {'sentiment_label': 'negative', 'text_of_tweet': 'Americans falsely assume that a war with China will be fought in China.<br><br>.</p>&mdash; david kersten (@davidkersten)', 'tweet_url': 'https://twitter.com/davidkersten/status/1631469854308827137?ref_src=twsrc%5Etfw'}\n",
      "22 {'sentiment_label': 'neutral', 'text_of_tweet': 'The boundary issue should be put in the proper place in bilateral relations, Qin said, adding that the situation on the borders should be brought under normalized management as soon as possible: China statement on EAM-China FM meet</p>&mdash; Sidhant Sibal (@sidhant)', 'tweet_url': 'https://twitter.com/sidhant/status/1631601051064467457?ref_src=twsrc%5Etfw'}\n",
      "23 {'sentiment_label': 'negative', 'text_of_tweet': '#China‚Äôs coming for us. This is war. <a href=\"https://twitter.com/hashtag/CCP?src=hash&amp;ref_src=twsrc%5Etfw\">#CCP</a></p>&mdash; Gordon G. Chang (@GordonGChang)', 'tweet_url': 'https://twitter.com/GordonGChang/status/1631460454601043968?ref_src=twsrc%5Etfw'}\n",
      "24 {'sentiment_label': 'negative', 'text_of_tweet': 'One of the many ongoing failures of west and particularly the US is this completely flawed belief that China wants to be a hegemonic power and that this view is shared and demanded by the Chinese people.</p>&mdash; The Sirius Report (@thesiriusreport)', 'tweet_url': 'https://twitter.com/thesiriusreport/status/1631558205124771841?ref_src=twsrc%5Etfw'}\n",
      "25 {'sentiment_label': 'negative', 'text_of_tweet': 'If Australia becomes &quot;Aboriginalia&quot; when we cede sovereignty to the elite militant aborigines, how will they defend the country against the Chinese invasion when it comes? Will they point sticks and throw stones at China&#39;s nuclear arsenal? <a href=\"https://twitter.com/hashtag/voteNO?src=hash&amp;ref_src=twsrc%5Etfw\">#voteNO</a></p>&mdash; Francis_Young (@commonsense058)', 'tweet_url': 'https://twitter.com/commonsense058/status/1631560666103566336?ref_src=twsrc%5Etfw'}\n",
      "26 {'sentiment_label': 'negative', 'text_of_tweet': '@GordonGChang</a> tells One America News China lied about the coronavirus from the beginning. One America‚Äôs John Hines has more from CPAC. [VIDEO] <a href=\"https://twitter.com/hashtag/ChinaLiedPeopleDied?src=hash&amp;ref_src=twsrc%5Etfw\">#ChinaLiedPeopleDied</a> <a href=\"https://twitter.com/hashtag/ChinaOwnsBiden?src=hash&amp;ref_src=twsrc%5Etfw\">#ChinaOwnsBiden</a> <a href=\"https://t.co/px1dNsHEeZ\">https://t.co/px1dNsHEeZ</a></p>&mdash; Jenny 1776üá∫üá∏ (@realouMAGAgirl)', 'tweet_url': 'https://twitter.com/realouMAGAgirl/status/1631638732783730691?ref_src=twsrc%5Etfw'}\n",
      "27 {'sentiment_label': 'neutral', 'text_of_tweet': 'Chinese aerospace <br>engineers used &#13;science developed by an American &#13;hypersonic scientist and a National &#13;Aeronautics Space Administration &#13;(NASA) project to address an issue with &#13;a proposed hypersonic-speed launch &#13;vehicle meant to intercept hypersonic &#13;missiles.<a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> <a href=\"https://t.co/Y8h5OCsQyG\">pic.twitter.com/Y8h5OCsQyG</a></p>&mdash; Hira Bashir (@HiraBK5090)', 'tweet_url': 'https://twitter.com/HiraBK5090/status/1631545302250299393?ref_src=twsrc%5Etfw'}\n",
      "28 {'sentiment_label': 'positive', 'text_of_tweet': 'China dominates global tech race. Beijing has a ‚Äústunning lead‚Äù over the US.<br><br>China is leading the world in 37 out of 44 critical and emerging technologies, the Australian Strategic Policy Institute (ASPI) said.<br><br>‚ÄûBeijing is the world‚Äôs leading science and technology superpower.‚Äú</p>&mdash; Make Peace Now; alternative news (@AlternatNews)', 'tweet_url': 'https://twitter.com/AlternatNews/status/1631606264189919232?ref_src=twsrc%5Etfw'}\n",
      "29 {'sentiment_label': 'negative', 'text_of_tweet': 'It appears as though as the tables are turning, it will be the west starved for resources while many of the nations with plentiful resources are gravitating to Russia and China...<br><br>Sudan is ready to cooperate with Russia on oil production issues.<br><br>The head of the Sudan Energy and‚Ä¶ <a href=\"https://t.co/HsDWesE4h5\">https://t.co/HsDWesE4h5</a></p>&mdash; GraphicW (@GraphicW5)', 'tweet_url': 'https://twitter.com/GraphicW5/status/1631657452134440963?ref_src=twsrc%5Etfw'}\n",
      "30 {'sentiment_label': 'positive', 'text_of_tweet': 'Yuqi‚Äôs stylist in China is always on point! They never miss! <a href=\"https://t.co/lSoHJLHxzP\">pic.twitter.com/lSoHJLHxzP</a></p>&mdash; Singer Xiao Song | Little Giant | Yuqi (@yuqiriiin)', 'tweet_url': 'https://twitter.com/yuqiriiin/status/1631603822203383809?ref_src=twsrc%5Etfw'}\n",
      "31 {'sentiment_label': 'neutral', 'text_of_tweet': 'I‚Äôm currently working in China. Almost exactly 100 years ago my great grandfather was here. These are his watercolours he sent home to his son (my grandfather). <a href=\"https://twitter.com/hashtag/History?src=hash&amp;ref_src=twsrc%5Etfw\">#History</a> <a href=\"https://t.co/sipek5usa8\">pic.twitter.com/sipek5usa8</a></p>&mdash; Dr Sam Willis (@DrSamWillis)', 'tweet_url': 'https://twitter.com/DrSamWillis/status/1631487477780213760?ref_src=twsrc%5Etfw'}\n",
      "32 {'sentiment_label': 'positive', 'text_of_tweet': 'Russia&#39;s energy policy will rely on reliable partners, including China and India, but not the West.<br> Russia will not allow the West to &quot;blow up gas pipelines&quot; again -<br> Lavrov</p>&mdash; Enrico60üá®üá≥üá∑üá∫Ôºà‰∫ífoÔºâ (@enfree1993)', 'tweet_url': 'https://twitter.com/enfree1993/status/1631569420278726661?ref_src=twsrc%5Etfw'}\n",
      "33 {'sentiment_label': 'negative', 'text_of_tweet': 'Iranian opposition: Iran is too close to China/Russia, and that&#39;s why the US hates us.<br><br>Russian opposition: Russia is too close to Iran/China, and that&#39;s why the US hates us.<br><br>Chinese opposition: China is too close to Russia/Iran, and that&#39;s why the US hates us.<br><br>LOL.</p>&mdash; DaiWW (@BeijingDai)', 'tweet_url': 'https://twitter.com/BeijingDai/status/1631569408484323328?ref_src=twsrc%5Etfw'}\n",
      "34 {'sentiment_label': 'positive', 'text_of_tweet': 'Justin Trudeau has a level of admiration for China&#39;s money.</p>&mdash; Zachary Tisdale üá®üá¶ (@ztisdale)', 'tweet_url': 'https://twitter.com/ztisdale/status/1631462637031632898?ref_src=twsrc%5Etfw'}\n",
      "35 {'sentiment_label': 'negative', 'text_of_tweet': 'It seems that not only does <a href=\"https://twitter.com/JustinTrudeau?ref_src=twsrc%5Etfw\">@JustinTrudeau</a> have an admiration for the basic dictatorship of China‚Ä¶<br><br>He also has their financing.<a href=\"https://twitter.com/hashtag/ChinaTrudeau?src=hash&amp;ref_src=twsrc%5Etfw\">#ChinaTrudeau</a></p>&mdash; Viva Frei (@thevivafrei)', 'tweet_url': 'https://twitter.com/thevivafrei/status/1631466024158519298?ref_src=twsrc%5Etfw'}\n",
      "36 {'sentiment_label': 'negative', 'text_of_tweet': 'Russia is getting their dick kicked in Ukraine the one thing China and Russia have in common are paper tiger armies that are way over hyped and rife with corruption <a href=\"https://t.co/A7bnnidRDK\">https://t.co/A7bnnidRDK</a></p>&mdash; Toriel1one1 (@toriel1one1)', 'tweet_url': 'https://twitter.com/toriel1one1/status/1631497804345483264?ref_src=twsrc%5Etfw'}\n",
      "37 {'sentiment_label': 'negative', 'text_of_tweet': 'üá∫üá∏üá®üá≥‚ò¢Ô∏è‚öõÔ∏è&quot;US is the main source of the nuclear threat in the world, they are hyping the theory of the threat from China in search of an excuse to expand their arsenal.&quot; - Chinese Foreign Ministry</p>&mdash; AZ üõ∞üåèüåçüåé (@AZgeopolitics)', 'tweet_url': 'https://twitter.com/AZgeopolitics/status/1631564558388043776?ref_src=twsrc%5Etfw'}\n",
      "38 {'sentiment_label': 'negative', 'text_of_tweet': 'Man do I have to stop myself from cringing when Lavrov talks.<br><br>Sign of the times really. Outside of energy, parts of defence &amp; a desire to contain China, there is nothing in the relationship anymore.<br><br>Long term stagnation is best case scenario.</p>&mdash; Yew&#39;s Finest (@FinestYew)', 'tweet_url': 'https://twitter.com/FinestYew/status/1631660098958540800?ref_src=twsrc%5Etfw'}\n",
      "39 {'sentiment_label': 'neutral', 'text_of_tweet': '#Flash</a> China has given a fresh loan of USD 700 million to Pakistan at the rate of 8.9%. Two railway stations of Pakistan (Lahore &amp; Sukkur) have been taken by China as security for 99 years or till the full and final payment of this loan, which is earlier. (Sources)</p>&mdash; Baba Banaras‚Ñ¢ (@RealBababanaras)', 'tweet_url': 'https://twitter.com/RealBababanaras/status/1631497938596945920?ref_src=twsrc%5Etfw'}\n",
      "40 {'sentiment_label': 'positive', 'text_of_tweet': '#China</a> leading <a href=\"https://twitter.com/hashtag/US?src=hash&amp;ref_src=twsrc%5Etfw\">#US</a> in technology race in all but a few fields, thinktank finds<br><br>Year-long study finds China leads in 37 of 44 areas it tracked, with potential for a monopoly in areas such as nanoscale materials and synthetic biology.<a href=\"https://t.co/IICGKLrDOM\">https://t.co/IICGKLrDOM</a></p>&mdash; Indo-Pacific News - Geo-Politics &amp; Military News (@IndoPac_Info)', 'tweet_url': 'https://twitter.com/IndoPac_Info/status/1631589226478198784?ref_src=twsrc%5Etfw'}\n",
      "41 {'sentiment_label': 'positive', 'text_of_tweet': 'China&#39;s &#39;Two Sessions&#39; annual legislative body begins, here in Beijing, tomorrow.<br><br>With all eyes on China&#39;s top law making body, Reuters reports GDP goals may be set as high as 6% growth for 2023.<a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> <a href=\"https://twitter.com/hashtag/TwoSessions?src=hash&amp;ref_src=twsrc%5Etfw\">#TwoSessions</a><a href=\"https://t.co/uZSx67cgRV\">https://t.co/uZSx67cgRV</a></p>&mdash; Jason - ‰∏äÂÆòÊù∞Êñá (@ShangguanJiewen)', 'tweet_url': 'https://twitter.com/ShangguanJiewen/status/1631488736885178370?ref_src=twsrc%5Etfw'}\n",
      "42 {'sentiment_label': 'neutral', 'text_of_tweet': 'The Anti-Counterfeit Authority (ACA) has released goods worth Sh50 million that were seized at China Square.<br><br>The quick return of the goods comes a day after the Chinese embassy urged the Kenyan government to intervene to protect Chinese enterprises and citizens.<br><br>‚Äî Nation</p>&mdash; Moe (@moneyacademyKE)', 'tweet_url': 'https://twitter.com/moneyacademyKE/status/1631512472644632576?ref_src=twsrc%5Etfw'}\n",
      "43 {'sentiment_label': 'negative', 'text_of_tweet': 'Khan was ousted from power in April after losing a no-confidence vote in his leadership, which he alleged was part of a US-led conspiracy targeting him because of his independent foreign policy decisions on Russia, China and Afghanistan.<a href=\"https://twitter.com/7n_Star_?ref_src=twsrc%5Etfw\">@7n_Star_</a><a href=\"https://twitter.com/hashtag/%D8%AA%D8%A8%D8%A7%DB%81%DB%8C_%D8%B3%D8%B1%DA%A9%D8%A7%D8%B1_%D8%AC%D8%A7%D9%86_%DA%86%DA%BE%D9%88%DA%91%D9%88?src=hash&amp;ref_src=twsrc%5Etfw\">#ÿ™ÿ®ÿß€Å€å_ÿ≥ÿ±⁄©ÿßÿ±_ÿ¨ÿßŸÜ_⁄Ü⁄æŸà⁄ëŸà</a></p>&mdash; ùêçŒ± ù…±œÖ Ç  ÇŒ± ÖŒπ‘ã“Ω“Ω…≥üôÉ (@7n_Star_)', 'tweet_url': 'https://twitter.com/7n_Star_/status/1631597034254872577?ref_src=twsrc%5Etfw'}\n",
      "44 {'sentiment_label': 'negative', 'text_of_tweet': '#China</a> providing <a href=\"https://twitter.com/hashtag/Russia?src=hash&amp;ref_src=twsrc%5Etfw\">#Russia</a> uniforms, weapons and ammunition only prolongs the war in <a href=\"https://twitter.com/hashtag/Ukraine?src=hash&amp;ref_src=twsrc%5Etfw\">#Ukraine</a>. Russia has the bodies; China will outfit them. Not only will it prolong the war - but it also weakens Russia as well. Is that the plan? Who needs enemies when you <a href=\"https://t.co/pzUiyZATEi\">https://t.co/pzUiyZATEi</a>‚Ä¶ <a href=\"https://t.co/xVfdrqVlby\">https://t.co/xVfdrqVlby</a></p>&mdash; Jon Sweet (@JESweet2022)', 'tweet_url': 'https://twitter.com/JESweet2022/status/1631630908024401927?ref_src=twsrc%5Etfw'}\n",
      "45 {'sentiment_label': 'negative', 'text_of_tweet': 'Protests in Kenya against China.<br>People in Kenya think that Chinese projects in Kenya help Chinese companies but not workers in Kenya.<a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> <a href=\"https://twitter.com/hashtag/Chinaprotests?src=hash&amp;ref_src=twsrc%5Etfw\">#Chinaprotests</a> <a href=\"https://twitter.com/hashtag/Kenya?src=hash&amp;ref_src=twsrc%5Etfw\">#Kenya</a> <a href=\"https://t.co/qOZI6yyWwI\">pic.twitter.com/qOZI6yyWwI</a></p>&mdash; That is China (@2022_Lockdown)', 'tweet_url': 'https://twitter.com/2022_Lockdown/status/1631488665384779776?ref_src=twsrc%5Etfw'}\n",
      "46 {'sentiment_label': 'neutral', 'text_of_tweet': 'My latest for <a href=\"https://twitter.com/dw_hotspotasia?ref_src=twsrc%5Etfw\">@dw_hotspotasia</a>: As <a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a>&#39;s rubber-stamp parliament gathers in Beijing this weekend, President Xi Jinping is expected to officially kick off his third term. China&#39;s Communist party will likely initiate further institutional reform. <a href=\"https://t.co/8lbe9CJ2SO\">https://t.co/8lbe9CJ2SO</a></p>&mdash; William Yang (@WilliamYang120)', 'tweet_url': 'https://twitter.com/WilliamYang120/status/1631630614549118978?ref_src=twsrc%5Etfw'}\n",
      "47 {'sentiment_label': 'negative', 'text_of_tweet': 'Beijing has criticized Canberra for blocking a bid by a Chinese-linked company to boost its ownership in a rare earths supplier, an episode that underscores the challenges the two nations face repairing ties <a href=\"https://t.co/1zbM0OKNgi\">https://t.co/1zbM0OKNgi</a></p>&mdash; Bloomberg (@business)', 'tweet_url': 'https://twitter.com/business/status/1631602420357758977?ref_src=twsrc%5Etfw'}\n",
      "48 {'sentiment_label': 'negative', 'text_of_tweet': 'The return of China‚Äôs top basketball league to its normal season format following years of Covid disruptions has been marred in controversy <a href=\"https://t.co/ufVfOYdDO0\">https://t.co/ufVfOYdDO0</a></p>&mdash; CNN (@CNN)', 'tweet_url': 'https://twitter.com/CNN/status/1631615109750554624?ref_src=twsrc%5Etfw'}\n",
      "49 {'sentiment_label': 'neutral', 'text_of_tweet': 'In meeting with Saudi FM Prince Faisal bin Farhan Al Saud, Chinese FM <a href=\"https://twitter.com/hashtag/QinGang?src=hash&amp;ref_src=twsrc%5Etfw\">#QinGang</a> said <a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> is ready to keep the positive momentum of high-level exchanges with <a href=\"https://twitter.com/hashtag/SaudiaArabia?src=hash&amp;ref_src=twsrc%5Etfw\">#SaudiaArabia</a> and work together to advance high-quality Belt and Road Cooperation. <a href=\"https://t.co/4A5v9ouAxy\">pic.twitter.com/4A5v9ouAxy</a></p>&mdash; Liu Yongfeng (@liupheonix)', 'tweet_url': 'https://twitter.com/liupheonix/status/1631473422818742272?ref_src=twsrc%5Etfw'}\n",
      "50 {'sentiment_label': 'positive', 'text_of_tweet': 'China ‚ÄòIs the Only One in the Race‚Äô to Make Electric Buses, Taxis and Trucks <a href=\"https://t.co/XF6UkHJ3Ur\">https://t.co/XF6UkHJ3Ur</a> by <a href=\"https://twitter.com/Trefor1?ref_src=twsrc%5Etfw\">@Trefor1</a> <a href=\"https://t.co/4VpWwZLmV7\">pic.twitter.com/4VpWwZLmV7</a></p>&mdash; CHINA (@china)', 'tweet_url': 'https://twitter.com/china/status/1069728152581218305?ref_src=twsrc%5Etfw'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader_model = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vader(textual_unit,\n",
    "              lemmatize=False,\n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "\n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "\n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(textual_unit)\n",
    "\n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-':\n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add)\n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.48      0.62        33\n",
      "     neutral       0.20      0.29      0.24         7\n",
      "    positive       0.19      0.40      0.26        10\n",
      "\n",
      "    accuracy                           0.44        50\n",
      "   macro avg       0.41      0.39      0.37        50\n",
      "weighted avg       0.62      0.44      0.49        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet)\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "\n",
    "# use scikit-learn's classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3a Answer Quantitative evaluation:\n",
    "Precision: The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.\n",
    "Recall: The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0.\n",
    "F1-score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "Support: The support is the number of occurrences of each class in y_true.\n",
    "Accuracy: The accuracy is the number of correctly classified samples divided by the total number of samples. The best value is 1 and the worst value is 0.\n",
    "Macro avg: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "Weighted avg: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‚Äòmacro‚Äô to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "Micro avg: Calculate metrics globally by counting the total true positives, false negatives and false positives. This is a better metric when we have class imbalance.\n",
    "Samples avg: Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n",
    "According to the classification report generated previously, it can be seen that the model has a high precision for the negative tweets, but low for the neutral and positive ones. This means that most things classified as negative are indeed negative, but that‚Äôs not the case for the positive and neutral tweets.\n",
    "Recall indicates how many relevant items are retrieved (e.g. how many of the negative items where classified as negative), the recall is low for all labels, being slightly higher for the negative (.48) and the lowest for the neutral (.39).\n",
    "The f1 score is relatively high for the negative, which makes sense since it had high precision and the highest recall out of the three, however the f1 is low for the negative and the neutral since both the precision and recall were low.\n",
    "Macro average for the precision is 0.41, while the weighted average is 0.69, the difference is due to the macro average not taking label imbalance into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified positive tweets: 6\n",
      "Number of misclassified negative tweets: 17\n",
      "Number of misclassified neutral tweets: 5\n"
     ]
    }
   ],
   "source": [
    "# error analysis\n",
    "misclassified_pos = []\n",
    "misclassified_neg = []\n",
    "misclassified_neu = []\n",
    "\n",
    "for i, (tweet, vader_label, gold_label) in enumerate(zip(tweets, all_vader_output, gold)):\n",
    "    if vader_label != gold_label:\n",
    "        if gold_label == 'positive':\n",
    "            misclassified_pos.append((i, tweet, vader_label, gold_label))\n",
    "        elif gold_label == 'negative':\n",
    "            misclassified_neg.append((i, tweet, vader_label, gold_label))\n",
    "        elif gold_label == 'neutral':\n",
    "            misclassified_neu.append((i, tweet, vader_label, gold_label))\n",
    "\n",
    "print('Number of misclassified positive tweets: {}'.format(len(misclassified_pos)))\n",
    "print('Number of misclassified negative tweets: {}'.format(len(misclassified_neg)))\n",
    "print('Number of misclassified neutral tweets: {}'.format(len(misclassified_neu)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: ‚ÄúcHiNa cAn‚ÄôT iNnOvAtE.‚Äù üí•Analysis by ASPI* shows that China leads the USA in whopping 37 out of 44 critical scientific areas such as AI, quantum computing, biotech, and advanced materials.<br><br>*funded by U.S. military industrial complex, so no pro-China bias <a href=\"https://t.co/CgNUmGA0iE\"> pic.twitter.com/CgNUmGA0iE\n",
      "Vader label: negative\n",
      "Gold label: positive\n",
      "-----------------------------\n",
      "Tweet: China has a prevalent weapon magazine culture which I can‚Äôt find in America. There are about 2 dozens of highly professional monthlies published and penned by the MIC itself covering every branch of the armed forces. You can buy these magazines at every street corner across the <a href=\"https://t.co/YVNteeP3Iq\">pic.twitter.com/YVNteeP3Iq</a></p>&mdash; Governor General (@manchuxi)\n",
      "Vader label: negative\n",
      "Gold label: positive\n",
      "-----------------------------\n",
      "Tweet: China has a &quot;stunning lead&quot; in 37 out of 44 critical and emerging technologies as Western democracies lose a global competition for research output, a security think tank said on Thursday after tracking defense, space, energy and biotechnology. <a href=\"https://t.co/icY1FHvVGK\">https://t.co/icY1FHvVGK</a></p>&mdash; NEWSMAX (@NEWSMAX)\n",
      "Vader label: neutral\n",
      "Gold label: positive\n",
      "-----------------------------\n",
      "Tweet: Russia&#39;s energy policy will rely on reliable partners, including China and India, but not the West.<br> Russia will not allow the West to &quot;blow up gas pipelines&quot; again -<br> Lavrov</p>&mdash; Enrico60üá®üá≥üá∑üá∫Ôºà‰∫ífoÔºâ (@enfree1993)\n",
      "Vader label: negative\n",
      "Gold label: positive\n",
      "-----------------------------\n",
      "Tweet: #China</a> leading <a href=\"https://twitter.com/hashtag/US?src=hash&amp;ref_src=twsrc%5Etfw\">#US</a> in technology race in all but a few fields, thinktank finds<br><br>Year-long study finds China leads in 37 of 44 areas it tracked, with potential for a monopoly in areas such as nanoscale materials and synthetic biology.<a href=\"https://t.co/IICGKLrDOM\">https://t.co/IICGKLrDOM</a></p>&mdash; Indo-Pacific News - Geo-Politics &amp; Military News (@IndoPac_Info)\n",
      "Vader label: neutral\n",
      "Gold label: positive\n",
      "-----------------------------\n",
      "Tweet: China ‚ÄòIs the Only One in the Race‚Äô to Make Electric Buses, Taxis and Trucks <a href=\"https://t.co/XF6UkHJ3Ur\">https://t.co/XF6UkHJ3Ur</a> by <a href=\"https://twitter.com/Trefor1?ref_src=twsrc%5Etfw\">@Trefor1</a> <a href=\"https://t.co/4VpWwZLmV7\">pic.twitter.com/4VpWwZLmV7</a></p>&mdash; CHINA (@china)\n",
      "Vader label: neutral\n",
      "Gold label: positive\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# print misclassified positive tweets\n",
    "for i, tweet, vader_label, gold_label in misclassified_pos:\n",
    "    print('Tweet: {}'.format(tweet))\n",
    "    print('Vader label: {}'.format(vader_label))\n",
    "    print('Gold label: {}'.format(gold_label))\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b Answer\n",
    "Error Analysis on Positive Tweets:\n",
    "We found 6 positive tweets that were missclassified by VADER.\n",
    "\n",
    "For instance, the tweet contains information about China being powerful in certain areas of scientific research. The content is mostly possitive, but VADER classifies it as negative. This could be due to VADER just taking into account the words present on it's lexicon or the sarcastic comment in the beginning that says \"China can't innovate\", can't being possibly seen as negative. The tweet contain words such as bias and no that have a negative sentiment rating.\n",
    "\n",
    "The second tweet VADER classifies as negative, while Gold as positive, it's ambiguos by the text itself whether it's positive or negative but could be classified as negative due to the use of the word weapon and can't. We argued it was positive about China as they had something that was apparently desired by the person that they missed while being in the US.\n",
    "\n",
    "The third tweet is classified as negative mostly because it contains words such as \"lose\" to make a comparison. Arguably the text could be negative based on perspective but we chose to focus on the sentiment about China instead of Western disappointment at Chinese success.\n",
    "\n",
    "In four and five there is a combination of positive words with negations in complex sentence structures so that might explain by the tweets were missclassified.\n",
    "\n",
    "The last tweet is classifies as neutral, but then again the meaning of the text itself is ambiguous. Probably the words in the text are just neither positive nor negative in the VADER lexicon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: China appears to be requiring foreign law professors to submit their syllabuses to ensure they are following a doctrine pushed by President Xi Jinping <a href=\"https://t.co/SuSWhELiCx\">https://t.co/SuSWhELiCx</a></p>&mdash; Bloomberg (@business)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: The United States has added two subsidiaries of Chinese genetics company BGI to a trade blacklist over allegations it conducted genetic analysis and surveillance activities for Beijing, which Washington says was used to repress ethnic minorities in China <a href=\"https://t.co/siXR57whNs\">https://t.co/siXR57whNs</a></p>&mdash; CNN (@CNN)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: China is building six times more new coal plants than the rest of the world combined, new research shows <a href=\"https://t.co/zd7akk1eqV\">https://t.co/zd7akk1eqV</a></p>&mdash; ABC News (@abcnews)\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: China''s turn towards fascism is accelerating <a href=\"https://t.co/Bpoey4WnAz\">pic.twitter.com/Bpoey4WnAz</a></p>&mdash; Chinese History Expert (@chineseciv)\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: In response to US actions, China will take retaliatory measures to protect Chinese corporations ‚Äî Ministry of Commerce of the People&#39;s Republic of China</p>&mdash; AZ üõ∞üåèüåçüåé (@AZgeopolitics)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: Let me ask you, how long would a China Police Station last in the US, Great Britain, Australia, Japan France, New Zealand. And you know if there was a threat of election interference this would be investigated even before the public demand them to do so. ü§îüá®üá≥ is so inbedded\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: It's fascinating that our gov&#39;t suddenly admits all the facts about COVID&#39;s origin, now that China has decided to side with Russia.</p>&mdash; Shukri Abdirahman (@ShuForCongress)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: Folks, China got what they wanted from Harper. That 31-year trade deal. And they got to execute Canadians.<br><br>Trudeau is less biddable.<br><br>China wants the CPC back in office, so they&#39;ve set this up. <br><br>That&#39;s what&#39;s going on here, IMO.<a href=\"https://twitter.com/hashtag/cdnpoli?src=hash&amp;ref_src=twsrc%5Etfw\">#cdnpoli</a></p>&mdash; Timothy Anderson üíâüíâüíâüíâüíâüé∂ (@AndersonBooz)\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: Blinken‚Äô trip to Uzbekistan has only one purpose‚Ä¶ to sow the seeds of regime change that would allow the U.S. Empire to take control of the country in a few years time and turn it into a dagger on the side of China &amp; Russia.</p>&mdash; ÂÄ™ÊòéËææ (Ni Mingda) (@NiMingda_GG)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: There is ten times more evidence of Biden-China collusion than there ever was of Trump-Russia collusion.<br><br>The Hunter Biden laptop is a smoking gun.<br><br>When have the lamestream media brought this up? Where&#39;s the campaign surveillance? When&#39;s a Special Counsel going to investigate?</p>&mdash; Kyle Becker (@kylenabecker)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: üá®üá≥üá∫üá∏: The heat is turning up <br><br>&quot;We strongly oppose the sale of arms to Chinese Taiwan...<br>We demand that the US cease arms sales to Taiwan and cease military ties with the island.&quot; <br>The People&#39;s Liberation Army of China is always ready to strike back...&quot;<br>-spokesman Tan Kefei<br>--&gt;üëá</p>&mdash; David Roth-Lindberg (@RothLindberg)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: A report from the Australian Institute for Strategic Policy Research warns that China is achieving a significant advantage over the US and the West in the vast majority of critical and advanced technologies.<br><br>According to the report, China leads in 37 out of 44 technologies‚Ä¶ <a href=\"https://t.co/namahAiBT2\">https://t.co/namahAiBT2</a></p>&mdash; GraphicW (@GraphicW5)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: If Australia becomes &quot;Aboriginalia&quot; when we cede sovereignty to the elite militant aborigines, how will they defend the country against the Chinese invasion when it comes? Will they point sticks and throw stones at China&#39;s nuclear arsenal? <a href=\"https://twitter.com/hashtag/voteNO?src=hash&amp;ref_src=twsrc%5Etfw\">#voteNO</a></p>&mdash; Francis_Young (@commonsense058)\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: It appears as though as the tables are turning, it will be the west starved for resources while many of the nations with plentiful resources are gravitating to Russia and China...<br><br>Sudan is ready to cooperate with Russia on oil production issues.<br><br>The head of the Sudan Energy and‚Ä¶ <a href=\"https://t.co/HsDWesE4h5\">https://t.co/HsDWesE4h5</a></p>&mdash; GraphicW (@GraphicW5)\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: It seems that not only does <a href=\"https://twitter.com/JustinTrudeau?ref_src=twsrc%5Etfw\">@JustinTrudeau</a> have an admiration for the basic dictatorship of China‚Ä¶<br><br>He also has their financing.<a href=\"https://twitter.com/hashtag/ChinaTrudeau?src=hash&amp;ref_src=twsrc%5Etfw\">#ChinaTrudeau</a></p>&mdash; Viva Frei (@thevivafrei)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: Man do I have to stop myself from cringing when Lavrov talks.<br><br>Sign of the times really. Outside of energy, parts of defence &amp; a desire to contain China, there is nothing in the relationship anymore.<br><br>Long term stagnation is best case scenario.</p>&mdash; Yew&#39;s Finest (@FinestYew)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: Protests in Kenya against China.<br>People in Kenya think that Chinese projects in Kenya help Chinese companies but not workers in Kenya.<a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> <a href=\"https://twitter.com/hashtag/Chinaprotests?src=hash&amp;ref_src=twsrc%5Etfw\">#Chinaprotests</a> <a href=\"https://twitter.com/hashtag/Kenya?src=hash&amp;ref_src=twsrc%5Etfw\">#Kenya</a> <a href=\"https://t.co/qOZI6yyWwI\">pic.twitter.com/qOZI6yyWwI</a></p>&mdash; That is China (@2022_Lockdown)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# print misclassified negative tweets\n",
    "for i, tweet, vader_label, gold_label in misclassified_neg:\n",
    "    print('Tweet: {}'.format(tweet))\n",
    "    print('Vader label: {}'.format(vader_label))\n",
    "    print('Gold label: {}'.format(gold_label))\n",
    "    print('-----------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b Answer\n",
    "Error Analysis on Negative Tweets:\n",
    "\n",
    "NOTE: since we found more than 10 negative missclassified tweets we'll try to explain the results for 10 of them.\n",
    "\n",
    "* 1. Words have a negative meaning because of the context, VADER misses out on that. (These words don't have a negative sentiment rating in the lexicon)\n",
    "* 2. Again, probably misses out on the context interpretation of words.\n",
    "* 3. The tweet contains words with negative sentiment rating such as \"repress\", but other such as \"ethnical\" are positive. VADER is not able to gauge the overall meaning of the sentence in this tweet.\n",
    "* 5. The tweet contains words that are neutral according to the VADER lexicon\n",
    "* 6. Lexicon contains word \"fascist\" but not fascism, since we are using the words and not the lemmas it might be that it doesn't recognize it as negative.\n",
    "* 9. \"retaliatory\" not in lexicon, protect is positive.\n",
    "* 11. Sentence is ambiguous, VADER just looks at the valence of each word.\n",
    "* 13. \"fascinating\" has a positive sentiment rating. The words in the tweet just have a positive sentiment rating, VADER is not able to gauge the context.\n",
    "* 16. Negative due to political context, not to separate words, so it's missclassified by vader.\n",
    "* 17. Similar to 16, meaning depends on context and knowledge about the world and politics, which vader doesn't have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I‚Äôm currently working in China. Almost exactly 100 years ago my great grandfather was here. These are his watercolours he sent home to his son (my grandfather). <a href=\"https://twitter.com/hashtag/History?src=hash&amp;ref_src=twsrc%5Etfw\">#History</a> <a href=\"https://t.co/sipek5usa8\">pic.twitter.com/sipek5usa8</a></p>&mdash; Dr Sam Willis (@DrSamWillis)\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "-----------------------------\n",
      "Tweet: #Flash</a> China has given a fresh loan of USD 700 million to Pakistan at the rate of 8.9%. Two railway stations of Pakistan (Lahore &amp; Sukkur) have been taken by China as security for 99 years or till the full and final payment of this loan, which is earlier. (Sources)</p>&mdash; Baba Banaras‚Ñ¢ (@RealBababanaras)\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "-----------------------------\n",
      "Tweet: The Anti-Counterfeit Authority (ACA) has released goods worth Sh50 million that were seized at China Square.<br><br>The quick return of the goods comes a day after the Chinese embassy urged the Kenyan government to intervene to protect Chinese enterprises and citizens.<br><br>‚Äî Nation</p>&mdash; Moe (@moneyacademyKE)\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "-----------------------------\n",
      "Tweet: My latest for <a href=\"https://twitter.com/dw_hotspotasia?ref_src=twsrc%5Etfw\">@dw_hotspotasia</a>: As <a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a>&#39;s rubber-stamp parliament gathers in Beijing this weekend, President Xi Jinping is expected to officially kick off his third term. China&#39;s Communist party will likely initiate further institutional reform. <a href=\"https://t.co/8lbe9CJ2SO\">https://t.co/8lbe9CJ2SO</a></p>&mdash; William Yang (@WilliamYang120)\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "-----------------------------\n",
      "Tweet: In meeting with Saudi FM Prince Faisal bin Farhan Al Saud, Chinese FM <a href=\"https://twitter.com/hashtag/QinGang?src=hash&amp;ref_src=twsrc%5Etfw\">#QinGang</a> said <a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> is ready to keep the positive momentum of high-level exchanges with <a href=\"https://twitter.com/hashtag/SaudiaArabia?src=hash&amp;ref_src=twsrc%5Etfw\">#SaudiaArabia</a> and work together to advance high-quality Belt and Road Cooperation. <a href=\"https://t.co/4A5v9ouAxy\">pic.twitter.com/4A5v9ouAxy</a></p>&mdash; Liu Yongfeng (@liupheonix)\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# print misclassified neutral tweets\n",
    "for i, tweet, vader_label, gold_label in misclassified_neu:\n",
    "    print('Tweet: {}'.format(tweet))\n",
    "    print('Vader label: {}'.format(vader_label))\n",
    "    print('Gold label: {}'.format(gold_label))\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b Answer\n",
    " Error Analysis on Neutral Tweets:\n",
    "\n",
    "NOTE: since we found more than 10 negative missclassified tweets we'll try to explain the results for 10 of them.\n",
    "\n",
    "* 30. Classified as positive due to the word \"great\" before grandfather.\n",
    "* 38. Missclassified as positive due to words such as \"fresh\" that are positive in the lexicon.\n",
    "* 41. Meaning depends on the context or possible missclassification as positive because of the use of word goods.\n",
    "* 45. Missclassified as positive due to words that are positive in the lexicon maybe the word could be likely or the possible argument that \"kick off\" is a positive or \"exciting\" word.\n",
    "* 48. Missclassified as positive due to words that are positive in the lexicon (\"positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from sklearn.datasets import load_files\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_tweets_train = load_files(str(airline_tweets_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER (as it is) on the set of airline tweets Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.49      0.62        39\n",
      "     neutral       0.79      0.63      0.70        30\n",
      "    positive       0.52      0.90      0.66        31\n",
      "\n",
      "    accuracy                           0.66       100\n",
      "   macro avg       0.72      0.67      0.66       100\n",
      "weighted avg       0.74      0.66      0.66       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets\n",
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"VADER (as it is) on the set of airline tweets Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets after having lemmatized the text Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.62        39\n",
      "     neutral       0.74      0.57      0.64        30\n",
      "    positive       0.54      0.90      0.67        31\n",
      "\n",
      "    accuracy                           0.65       100\n",
      "   macro avg       0.69      0.66      0.65       100\n",
      "weighted avg       0.70      0.65      0.65       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets after having lemmatized the text\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), lemmatize=True)\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets after having lemmatized the text Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets with only adjectives Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.21      0.34        39\n",
      "     neutral       0.39      0.93      0.55        30\n",
      "    positive       0.80      0.52      0.63        31\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.73      0.55      0.51       100\n",
      "weighted avg       0.75      0.52      0.49       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only adjectives\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), parts_of_speech_to_consider={'ADJ'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only adjectives Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets with only adjectives and after having lemmatized the text Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.21      0.34        39\n",
      "     neutral       0.39      0.93      0.55        30\n",
      "    positive       0.80      0.52      0.63        31\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.73      0.55      0.51       100\n",
      "weighted avg       0.75      0.52      0.49       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), lemmatize=True, parts_of_speech_to_consider={'ADJ'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only adjectives and after having lemmatized the text Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets with only nouns Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.13      0.22        39\n",
      "     neutral       0.35      0.87      0.50        30\n",
      "    positive       0.45      0.29      0.35        31\n",
      "\n",
      "    accuracy                           0.40       100\n",
      "   macro avg       0.54      0.43      0.36       100\n",
      "weighted avg       0.57      0.40      0.35       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only nouns\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), parts_of_speech_to_consider={'NOUN'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only nouns Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets with only nouns and after having lemmatized the text Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.13      0.22        39\n",
      "     neutral       0.35      0.87      0.50        30\n",
      "    positive       0.45      0.29      0.35        31\n",
      "\n",
      "    accuracy                           0.40       100\n",
      "   macro avg       0.54      0.43      0.36       100\n",
      "weighted avg       0.57      0.40      0.35       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), lemmatize=True, parts_of_speech_to_consider={'NOUN'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only nouns and after having lemmatized the text Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets with only verbs Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.33      0.49        39\n",
      "     neutral       0.39      0.90      0.55        30\n",
      "    positive       0.65      0.35      0.46        31\n",
      "\n",
      "    accuracy                           0.51       100\n",
      "   macro avg       0.66      0.53      0.50       100\n",
      "weighted avg       0.68      0.51      0.50       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only verbs\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), parts_of_speech_to_consider={'VERB'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only verbs Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets with only verbs and after having lemmatized the text Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.41      0.55        39\n",
      "     neutral       0.37      0.83      0.52        30\n",
      "    positive       0.79      0.35      0.49        31\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.67      0.53      0.52       100\n",
      "weighted avg       0.68      0.52      0.52       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), lemmatize=True, parts_of_speech_to_consider={'VERB'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only verbs and after having lemmatized the text Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 4 Answer\n",
    "If we compare the results of the first two experiments we can see that where all parts of speech are considered the difference between accuracy is minimal however  precision for negative and neutral tweets are higher in this case without lemmatization. This is because lemmatization could be removing some of the context of the word and therefore the sentiment of the word. In negative tweet recall, positive tweet precision and the positive tweet f1-score the lemmatized data prodced higher scores but also only by amounts between 0.1-0.2.\n",
    "Lemmatization makes the most difference in scores when considering only the verb part of speech. One can assume becasue this removes the verbs conjugation. One can see the precision for negative tweets drops by almost 0.10 when the data is lemmatized, along with the precision and recall for neutral tweets. However, the F1-score for negative tweets and positive tweets increase after lemmatization so if one considers the weighted averages as a metric then the lemmatized data is marginally better.\n",
    "\n",
    "In terms of the importance of the different parts of speech one could consider the accuracy and macro and weighted averages. When considering all parts of speech the overall accuracy is 0.66 and the weighted average is 0.74. When considering only verbs the accuracy is 0.52 and the weighted average is 0.68. When considering only nouns the accuracy is 0.40 and the weighted average is 0.57. When considering only adjectives the accuracy is 0.52 and the weighted average is 0.75. Therefore, one can see that the most important part of speech is the adjective. This is because the accuracy and weighted average are the highest besides when filtering for a part of speech. This is because adjectives are often used to describe the sentiment of a tweet. For example, if a tweet is positive it will often contain words such as \"great\" or \"amazing\". If a tweet is negative it will often contain words such as \"terrible\" or \"awful\". Therefore, adjectives are often used to describe the sentiment of a tweet and therefore are the most important part of speech for sentiment analysis. In some regards considering only adjectives performed better than all parts of speech but not in overall accuracy which is very interesting. What one could continue to do is consider parts of speech in combination with one another. For example, one could consider only nouns and adjectives or only verbs and adjectives. This could be interesting to see if the accuracy and weighted average increase or decrease to form a more concrete ranking of part of speech importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (Bag of words representation, min_df=2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.89      0.87       339\n",
      "     neutral       0.86      0.74      0.80       309\n",
      "    positive       0.81      0.88      0.85       303\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.84      0.84       951\n",
      "weighted avg       0.84      0.84      0.84       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "airline_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "#  bag of words representation of the airline tweets\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the bag of words representation of the tweets\n",
    "    airline_tweets_train.target, # the category values for each tweet\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "print(\"Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (Bag of words representation, min_df=2)\")\n",
    "print(classification_report(y_test, y_pred, target_names=airline_tweets_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.89      0.84       345\n",
      "     neutral       0.84      0.65      0.73       313\n",
      "    positive       0.79      0.87      0.83       293\n",
      "\n",
      "    accuracy                           0.80       951\n",
      "   macro avg       0.81      0.80      0.80       951\n",
      "weighted avg       0.81      0.80      0.80       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF representation of the airline tweets\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "docs_train2, docs_test2, y_train2, y_test2 = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "clf2 = MultinomialNB().fit(docs_train2, y_train2)\n",
    "y_pred2 = clf2.predict(docs_test2)\n",
    "\n",
    "print(\"Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\")\n",
    "print(classification_report(y_test2, y_pred2, target_names=airline_tweets_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.91      0.85       339\n",
      "     neutral       0.81      0.70      0.75       316\n",
      "    positive       0.86      0.84      0.85       296\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.82      0.82       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF representation of the airline tweets with min_df=5\n",
    "airline_vec = CountVectorizer(min_df=5, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "#  bag of words representation of the airline tweets\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "docs_train3, docs_test3, y_train3, y_test3 = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "clf3 = MultinomialNB().fit(docs_train3, y_train3)\n",
    "y_pred3 = clf3.predict(docs_test3)\n",
    "\n",
    "print(\"Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=5)\")\n",
    "print(classification_report(y_test3, y_pred3, target_names=airline_tweets_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=10)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.89      0.87       349\n",
      "     neutral       0.83      0.73      0.78       327\n",
      "    positive       0.79      0.85      0.82       275\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.82       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF representation of the airline tweets with min_df=10\n",
    "airline_vec = CountVectorizer(min_df=10, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "#  bag of words representation of the airline tweets\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "docs_train4, docs_test4, y_train4, y_test4 = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "clf4 = MultinomialNB().fit(docs_train4, y_train4)\n",
    "y_pred4 = clf4.predict(docs_test4)\n",
    "\n",
    "print(\"Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=10)\")\n",
    "print(classification_report(y_test4, y_pred4, target_names=airline_tweets_train.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 5 Answer:\n",
    "When comparing the two tweet prepresentations (bag of words and TF-IDF) we see that suprisingly the bag of word representation performs better. One woudl expect the TF-IDF representaiton to perform better because the TF-IDF representation takes into account the frequency and importance of the words in the tweets. However, one can see as the frequency threshold increases so does the accuracy of the sentiment analysis for the tweets in TF-IDF. Perhaps if we would continue increasing this threshold TF-IDF would be more effective than the bag of words representation. Bag of words ended with an accuracy of 0.84 while TF-IDF ended with an accuracy of 0.80. This is a difference of 0.04. This is not a large difference but it is still a difference when both had a frequency threshold of 2. As we increased the frequency threshold from 2 to 5 to 10 the accuracy increased from 0.80 to 0.82 and then to 0.83 which is comparable to the bag of words representation accuracy. The scores seem to be at least a bit effected by the frequency threshold but not very significantly. For further investigation one should most like increase the frequency threshold to see if the TF-IDF representation would outperform the bag of words representation and increase the frequency with bag of words and see what happens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 1521.0 @\n",
      "0 1401.0 united\n",
      "0 1264.0 .\n",
      "0 426.0 ``\n",
      "0 407.0 flight\n",
      "0 385.0 ?\n",
      "0 373.0 !\n",
      "0 311.0 #\n",
      "0 230.0 n't\n",
      "0 159.0 ''\n",
      "0 138.0 's\n",
      "0 117.0 service\n",
      "0 104.0 virginamerica\n",
      "0 100.0 :\n",
      "0 98.0 get\n",
      "0 96.0 customer\n",
      "0 95.0 cancelled\n",
      "0 91.0 delayed\n",
      "0 91.0 bag\n",
      "0 80.0 time\n",
      "0 79.0 plane\n",
      "0 79.0 'm\n",
      "0 74.0 hours\n",
      "0 74.0 ...\n",
      "0 69.0 still\n",
      "0 68.0 -\n",
      "0 66.0 gate\n",
      "0 66.0 ;\n",
      "0 65.0 http\n",
      "0 65.0 hour\n",
      "0 64.0 late\n",
      "0 64.0 airline\n",
      "0 61.0 would\n",
      "0 59.0 &\n",
      "0 56.0 help\n",
      "0 54.0 one\n",
      "0 54.0 2\n",
      "0 53.0 delay\n",
      "0 53.0 ca\n",
      "0 53.0 amp\n",
      "0 52.0 like\n",
      "0 50.0 $\n",
      "0 49.0 worst\n",
      "0 47.0 flights\n",
      "0 46.0 waiting\n",
      "0 46.0 never\n",
      "0 45.0 flightled\n",
      "0 44.0 us\n",
      "0 43.0 fly\n",
      "0 43.0 3\n",
      "0 42.0 've\n",
      "0 40.0 wait\n",
      "0 39.0 really\n",
      "0 39.0 lost\n",
      "0 39.0 ever\n",
      "0 39.0 (\n",
      "0 38.0 back\n",
      "0 37.0 thanks\n",
      "0 37.0 due\n",
      "0 37.0 bags\n",
      "0 36.0 u\n",
      "0 36.0 check\n",
      "0 35.0 ticket\n",
      "0 35.0 day\n",
      "0 34.0 trying\n",
      "0 34.0 seat\n",
      "0 34.0 people\n",
      "0 34.0 )\n",
      "0 33.0 crew\n",
      "0 33.0 another\n",
      "0 32.0 luggage\n",
      "0 32.0 even\n",
      "0 32.0 airport\n",
      "0 32.0 4\n",
      "0 31.0 problems\n",
      "0 30.0 staff\n",
      "0 30.0 seats\n",
      "0 29.0 last\n",
      "0 28.0 today\n",
      "0 28.0 phone\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 1406.0 @\n",
      "1 498.0 ?\n",
      "1 490.0 .\n",
      "1 313.0 jetblue\n",
      "1 294.0 :\n",
      "1 258.0 southwestair\n",
      "1 253.0 united\n",
      "1 253.0 ``\n",
      "1 239.0 flight\n",
      "1 220.0 #\n",
      "1 191.0 americanair\n",
      "1 186.0 http\n",
      "1 177.0 !\n",
      "1 160.0 usairways\n",
      "1 133.0 's\n",
      "1 87.0 get\n",
      "1 77.0 virginamerica\n",
      "1 77.0 ''\n",
      "1 71.0 -\n",
      "1 63.0 flights\n",
      "1 62.0 please\n",
      "1 62.0 )\n",
      "1 54.0 need\n",
      "1 53.0 (\n",
      "1 52.0 help\n",
      "1 49.0 n't\n",
      "1 45.0 dm\n",
      "1 43.0 would\n",
      "1 43.0 ;\n",
      "1 41.0 us\n",
      "1 40.0 ...\n",
      "1 38.0 ‚Äù\n",
      "1 37.0 ‚Äú\n",
      "1 36.0 fleet\n",
      "1 36.0 fleek\n",
      "1 36.0 &\n",
      "1 35.0 tomorrow\n",
      "1 35.0 flying\n",
      "1 34.0 way\n",
      "1 34.0 hi\n",
      "1 34.0 'm\n",
      "1 33.0 thanks\n",
      "1 33.0 know\n",
      "1 30.0 change\n",
      "1 30.0 cancelled\n",
      "1 29.0 one\n",
      "1 29.0 number\n",
      "1 29.0 like\n",
      "1 27.0 fly\n",
      "1 26.0 time\n",
      "1 26.0 could\n",
      "1 26.0 amp\n",
      "1 25.0 today\n",
      "1 25.0 check\n",
      "1 24.0 new\n",
      "1 23.0 travel\n",
      "1 23.0 see\n",
      "1 23.0 guys\n",
      "1 23.0 destinationdragons\n",
      "1 23.0 airport\n",
      "1 22.0 go\n",
      "1 20.0 tickets\n",
      "1 20.0 sent\n",
      "1 20.0 next\n",
      "1 20.0 going\n",
      "1 20.0 back\n",
      "1 19.0 use\n",
      "1 19.0 ceo\n",
      "1 18.0 want\n",
      "1 18.0 ticket\n",
      "1 18.0 follow\n",
      "1 18.0 add\n",
      "1 18.0 2\n",
      "1 17.0 weather\n",
      "1 17.0 trying\n",
      "1 17.0 question\n",
      "1 17.0 passengers\n",
      "1 17.0 make\n",
      "1 16.0 start\n",
      "1 16.0 service\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 1324.0 @\n",
      "2 1046.0 !\n",
      "2 772.0 .\n",
      "2 310.0 #\n",
      "2 302.0 southwestair\n",
      "2 283.0 thanks\n",
      "2 283.0 jetblue\n",
      "2 245.0 united\n",
      "2 242.0 thank\n",
      "2 227.0 ``\n",
      "2 174.0 americanair\n",
      "2 169.0 flight\n",
      "2 168.0 :\n",
      "2 136.0 usairways\n",
      "2 133.0 great\n",
      "2 94.0 )\n",
      "2 87.0 service\n",
      "2 75.0 virginamerica\n",
      "2 73.0 guys\n",
      "2 72.0 http\n",
      "2 70.0 love\n",
      "2 66.0 much\n",
      "2 65.0 best\n",
      "2 64.0 's\n",
      "2 59.0 awesome\n",
      "2 59.0 ;\n",
      "2 58.0 customer\n",
      "2 52.0 -\n",
      "2 48.0 time\n",
      "2 48.0 amazing\n",
      "2 47.0 good\n",
      "2 42.0 got\n",
      "2 41.0 n't\n",
      "2 41.0 airline\n",
      "2 40.0 us\n",
      "2 40.0 help\n",
      "2 39.0 &\n",
      "2 36.0 get\n",
      "2 36.0 ...\n",
      "2 35.0 crew\n",
      "2 34.0 today\n",
      "2 34.0 gate\n",
      "2 33.0 appreciate\n",
      "2 33.0 amp\n",
      "2 31.0 fly\n",
      "2 30.0 ''\n",
      "2 28.0 see\n",
      "2 28.0 home\n",
      "2 27.0 made\n",
      "2 27.0 flying\n",
      "2 26.0 response\n",
      "2 26.0 first\n",
      "2 26.0 ever\n",
      "2 26.0 'm\n",
      "2 25.0 work\n",
      "2 25.0 back\n",
      "2 25.0 always\n",
      "2 25.0 (\n",
      "2 24.0 new\n",
      "2 24.0 like\n",
      "2 23.0 well\n",
      "2 23.0 tonight\n",
      "2 23.0 nice\n",
      "2 23.0 day\n",
      "2 22.0 would\n",
      "2 22.0 u\n",
      "2 22.0 team\n",
      "2 22.0 ?\n",
      "2 22.0 'll\n",
      "2 21.0 southwest\n",
      "2 21.0 know\n",
      "2 21.0 job\n",
      "2 21.0 flights\n",
      "2 20.0 yes\n",
      "2 20.0 please\n",
      "2 19.0 plane\n",
      "2 19.0 follow\n",
      "2 19.0 agent\n",
      "2 18.0 staff\n",
      "2 18.0 really\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "\n",
    "airline_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "#  bag of words representation of the airline tweets\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the bag of words model\n",
    "    airline_tweets_train.target, # the category values for each tweet\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 Answer:\n",
    "which features did you expect for each separate class and why?\n",
    "Which features did you not expect and why ?\n",
    " The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why?\n",
    "\n",
    "In the section important words in negative documents we see a high feature count of punctuation and the tweet @ symbol and some expected airplane related words like united(the airline) and flight. What one sees that is also to be expected in the negative is the contraction n't, a negation and words like cancelled, delayed, late which are flight and domain negative concepts. We also see negative adjectives and adverbs like worst, and never. What one would not usually xpect in this section is the \"0 37.0 thanks\" we see but that could be because thanks can be used in a condescending, sarcastic or negative tone such as \"this was the worst service thanks to incompetent staff\" or something.\n",
    "In the section important words in positive tweets we see also a lot of punctuation and the @ symbol. Interestingly like the negative tweet sections thanks is one of the words high in feature count but unsurprisingly with a much higher count than in the negetive tweet section, almost 10 times higher. We also see positive adjectives and adverbs like great, best, and good. We also see positive words like love, thanks, and yes. These are all words that are used in a positive context and were thus to be expected. In this section there are not really any words that do not fit expectations.\n",
    "In the section important words in neutral tweets we see a lot of punctuation and the @ symbol. We also see words like flight, united, jetblue and other airline related words. These are all words that are used in a neutral context and were thus to be expected. However, in this section we can also see some words that one would expect more in the other two sections such as cancelled, and thanks. These words are not necessarily neutral but they could possibly be used in a neutral context and are not counted as high as in the negative and positve sections. For example, \"I cancelled my flight because it was delayed\" is a negative tweet but \"I cancelled my flight because I had to go to the hospital\" is a neutral tweet.\n",
    " The list contains all kinds of words such as names of airlines, punctuation, numbers and content words and some can definitely be removed without too much effect we believe. For instance, all the punctuation and @ symbol specifically that are very high in count in all three sections would most likely make no difference to the sentiment analysis if removed. Possibly with the only exception being ! which can be used to express emotion arguably more than a lot of other punctuation. We also believe that the numbers could be removed as they are not really words and are not really used in a context that would be relevant to sentiment analysis. We also believe that possibly if we are not searching for sentiments in regard to or in connection with specific airlines the names of airlines could be removed as they are not as relevant and appear in all three sections. One must however be careful about removing contextual flight related words that have sentiment attached like cancelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
