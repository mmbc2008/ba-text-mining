{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'positive', 'text_of_tweet': '‚ÄúcHiNa cAn‚ÄôT iNnOvAtE.‚Äù üí•Analysis by ASPI* shows that China leads the USA in whopping 37 out of 44 critical scientific areas such as AI, quantum computing, biotech, and advanced materials.<br><br>*funded by U.S. military industrial complex, so no pro-China bias <a href=\"https://t.co/CgNUmGA0iE\"> pic.twitter.com/CgNUmGA0iE', 'tweet_url': 'https://twitter.com/Kanthan2030/status/1631622840989675520?ref_src=twsrc%5Etfw'}\n",
      "2 {'sentiment_label': 'negative', 'text_of_tweet': 'AMERICAN WAR MACHINE NOW FOCUSED ON CHINA<br> <a href=\"https://t.co/5zUMGxoXNQ\">pic.twitter.com/5zUMGxoXNQ</a></p>&mdash; The_Real_Fly (@The_Real_Fly)', 'tweet_url': 'https://twitter.com/The_Real_Fly/status/1631542150675529729?ref_src=twsrc%5Etfw'}\n",
      "3 {'sentiment_label': 'negative', 'text_of_tweet': 'China appears to be requiring foreign law professors to submit their syllabuses to ensure they are following a doctrine pushed by President Xi Jinping <a href=\"https://t.co/SuSWhELiCx\">https://t.co/SuSWhELiCx</a></p>&mdash; Bloomberg (@business)', 'tweet_url': 'https://twitter.com/business/status/1631576391954169857?ref_src=twsrc%5Etfw'}\n",
      "4 {'sentiment_label': 'negative', 'text_of_tweet': 'The United States has added two subsidiaries of Chinese genetics company BGI to a trade blacklist over allegations it conducted genetic analysis and surveillance activities for Beijing, which Washington says was used to repress ethnic minorities in China <a href=\"https://t.co/siXR57whNs\">https://t.co/siXR57whNs</a></p>&mdash; CNN (@CNN)', 'tweet_url': 'https://twitter.com/CNN/status/1631622994924544001?ref_src=twsrc%5Etfw'}\n",
      "5 {'sentiment_label': 'positive', 'text_of_tweet': 'China has a prevalent weapon magazine culture which I can‚Äôt find in America. There are about 2 dozens of highly professional monthlies published and penned by the MIC itself covering every branch of the armed forces. You can buy these magazines at every street corner across the <a href=\"https://t.co/YVNteeP3Iq\">pic.twitter.com/YVNteeP3Iq</a></p>&mdash; Governor General (@manchuxi)', 'tweet_url': 'https://twitter.com/manchuxi/status/1631534583475830788?ref_src=twsrc%5Etfw'}\n",
      "6 {'sentiment_label': 'negative', 'text_of_tweet': 'China is building six times more new coal plants than the rest of the world combined, new research shows <a href=\"https://t.co/zd7akk1eqV\">https://t.co/zd7akk1eqV</a></p>&mdash; ABC News (@abcnews)', 'tweet_url': 'https://twitter.com/abcnews/status/1631450164375478272?ref_src=twsrc%5Etfw'}\n",
      "7 {'sentiment_label': 'negative', 'text_of_tweet': 'China\\'\\'s turn towards fascism is accelerating <a href=\"https://t.co/Bpoey4WnAz\">pic.twitter.com/Bpoey4WnAz</a></p>&mdash; Chinese History Expert (@chineseciv)', 'tweet_url': 'https://twitter.com/chineseciv/status/1631515516207788033?ref_src=twsrc%5Etfw'}\n",
      "8 {'sentiment_label': 'positive', 'text_of_tweet': 'China has a &quot;stunning lead&quot; in 37 out of 44 critical and emerging technologies as Western democracies lose a global competition for research output, a security think tank said on Thursday after tracking defense, space, energy and biotechnology. <a href=\"https://t.co/icY1FHvVGK\">https://t.co/icY1FHvVGK</a></p>&mdash; NEWSMAX (@NEWSMAX)', 'tweet_url': 'https://twitter.com/NEWSMAX/status/1631523549122007040?ref_src=twsrc%5Etfw'}\n",
      "9 {'sentiment_label': 'negative', 'text_of_tweet': \"I'm just wondering if there is any person in Taiwan who thinks that the Biden neocons are pumping billions of dollars of weapons onto their Island and antagonizing China to make them safer?</p>&mdash; Garland Nixon (@GarlandNixon)\", 'tweet_url': 'https://twitter.com/GarlandNixon/status/1631451970752978947?ref_src=twsrc%5Etfw'}\n",
      "10 {'sentiment_label': 'negative', 'text_of_tweet': 'In response to US actions, China will take retaliatory measures to protect Chinese corporations ‚Äî Ministry of Commerce of the People&#39;s Republic of China</p>&mdash; AZ üõ∞üåèüåçüåé (@AZgeopolitics)', 'tweet_url': 'https://twitter.com/AZgeopolitics/status/1631653133104345088?ref_src=twsrc%5Etfw'}\n",
      "11 {'sentiment_label': 'negative', 'text_of_tweet': 'Today is March 3, 2023 and Joe Biden is still an illegitimate President and is owned by China!</p>&mdash; PISSED OFF PATRIOT HOFFY üñï (@PATRIOTGHOFFY)', 'tweet_url': 'https://twitter.com/PATRIOTGHOFFY/status/1631645105684635648?ref_src=twsrc%5Etfw'}\n",
      "12 {'sentiment_label': 'negative', 'text_of_tweet': 'Let me ask you, how long would a China Police Station last in the US, Great Britain, Australia, Japan France, New Zealand. And you know if there was a threat of election interference this would be investigated even before the public demand them to do so. ü§îüá®üá≥ is so inbedded', 'tweet_url': 'https://t.co/Lfxx4UD0wg'}\n",
      "13 {'sentiment_label': 'negative', 'text_of_tweet': 'Wicked cleverness: China wages border aggression against India and then repeatedly advises India to not let the border situation come in the way of bilateral cooperation. China&#39;s latest statement says India should put the border issue in &quot;the proper place in bilateral relations.‚Äù</p>&mdash; Brahma Chellaney (@Chellaney)', 'tweet_url': 'https://twitter.com/Chellaney/status/1631610600781647872?ref_src=twsrc%5Etfw'}\n",
      "14 {'sentiment_label': 'negative', 'text_of_tweet': \"It's fascinating that our gov&#39;t suddenly admits all the facts about COVID&#39;s origin, now that China has decided to side with Russia.</p>&mdash; Shukri Abdirahman (@ShuForCongress)\", 'tweet_url': 'https://twitter.com/ShuForCongress/status/1631653770147889153?ref_src=twsrc%5Etfw'}\n",
      "15 {'sentiment_label': 'negative', 'text_of_tweet': 'The public is inching closer and closer to the harsh reality.<br>Russia and China‚Äôs displeasure with US biological activity in Ukraine, is because of Covid. <br>Western Criminals created SARS-CoV-2, which killed millions of people, and now the Eastern world is angry.</p>&mdash; D-Bark (@DBark46107258)', 'tweet_url': 'https://twitter.com/DBark46107258/status/1631650236279173120?ref_src=twsrc%5Etfw'}\n",
      "16 {'sentiment_label': 'negative', 'text_of_tweet': 'Folks, China got what they wanted from Harper. That 31-year trade deal. And they got to execute Canadians.<br><br>Trudeau is less biddable.<br><br>China wants the CPC back in office, so they&#39;ve set this up. <br><br>That&#39;s what&#39;s going on here, IMO.<a href=\"https://twitter.com/hashtag/cdnpoli?src=hash&amp;ref_src=twsrc%5Etfw\">#cdnpoli</a></p>&mdash; Timothy Anderson üíâüíâüíâüíâüíâüé∂ (@AndersonBooz)', 'tweet_url': 'https://twitter.com/AndersonBooz/status/1631545345556779009?ref_src=twsrc%5Etfw'}\n",
      "17 {'sentiment_label': 'negative', 'text_of_tweet': 'Blinken‚Äô trip to Uzbekistan has only one purpose‚Ä¶ to sow the seeds of regime change that would allow the U.S. Empire to take control of the country in a few years time and turn it into a dagger on the side of China &amp; Russia.</p>&mdash; ÂÄ™ÊòéËææ (Ni Mingda) (@NiMingda_GG)', 'tweet_url': 'https://twitter.com/NiMingda_GG/status/1631642321933484034?ref_src=twsrc%5Etfw'}\n",
      "18 {'sentiment_label': 'negative', 'text_of_tweet': 'There is ten times more evidence of Biden-China collusion than there ever was of Trump-Russia collusion.<br><br>The Hunter Biden laptop is a smoking gun.<br><br>When have the lamestream media brought this up? Where&#39;s the campaign surveillance? When&#39;s a Special Counsel going to investigate?</p>&mdash; Kyle Becker (@kylenabecker)', 'tweet_url': 'https://twitter.com/kylenabecker/status/1631654725367021569?ref_src=twsrc%5Etfw'}\n",
      "19 {'sentiment_label': 'negative', 'text_of_tweet': 'üá®üá≥üá∫üá∏: The heat is turning up <br><br>&quot;We strongly oppose the sale of arms to Chinese Taiwan...<br>We demand that the US cease arms sales to Taiwan and cease military ties with the island.&quot; <br>The People&#39;s Liberation Army of China is always ready to strike back...&quot;<br>-spokesman Tan Kefei<br>--&gt;üëá</p>&mdash; David Roth-Lindberg (@RothLindberg)', 'tweet_url': 'https://twitter.com/RothLindberg/status/1631635667154337794?ref_src=twsrc%5Etfw'}\n",
      "20 {'sentiment_label': 'negative', 'text_of_tweet': 'A report from the Australian Institute for Strategic Policy Research warns that China is achieving a significant advantage over the US and the West in the vast majority of critical and advanced technologies.<br><br>According to the report, China leads in 37 out of 44 technologies‚Ä¶ <a href=\"https://t.co/namahAiBT2\">https://t.co/namahAiBT2</a></p>&mdash; GraphicW (@GraphicW5)', 'tweet_url': 'https://twitter.com/GraphicW5/status/1631634185742868480?ref_src=twsrc%5Etfw'}\n",
      "21 {'sentiment_label': 'negative', 'text_of_tweet': 'Americans falsely assume that a war with China will be fought in China.<br><br>.</p>&mdash; david kersten (@davidkersten)', 'tweet_url': 'https://twitter.com/davidkersten/status/1631469854308827137?ref_src=twsrc%5Etfw'}\n",
      "22 {'sentiment_label': 'neutral', 'text_of_tweet': 'The boundary issue should be put in the proper place in bilateral relations, Qin said, adding that the situation on the borders should be brought under normalized management as soon as possible: China statement on EAM-China FM meet</p>&mdash; Sidhant Sibal (@sidhant)', 'tweet_url': 'https://twitter.com/sidhant/status/1631601051064467457?ref_src=twsrc%5Etfw'}\n",
      "23 {'sentiment_label': 'negative', 'text_of_tweet': '#China‚Äôs coming for us. This is war. <a href=\"https://twitter.com/hashtag/CCP?src=hash&amp;ref_src=twsrc%5Etfw\">#CCP</a></p>&mdash; Gordon G. Chang (@GordonGChang)', 'tweet_url': 'https://twitter.com/GordonGChang/status/1631460454601043968?ref_src=twsrc%5Etfw'}\n",
      "24 {'sentiment_label': 'negative', 'text_of_tweet': 'One of the many ongoing failures of west and particularly the US is this completely flawed belief that China wants to be a hegemonic power and that this view is shared and demanded by the Chinese people.</p>&mdash; The Sirius Report (@thesiriusreport)', 'tweet_url': 'https://twitter.com/thesiriusreport/status/1631558205124771841?ref_src=twsrc%5Etfw'}\n",
      "25 {'sentiment_label': 'negative', 'text_of_tweet': 'If Australia becomes &quot;Aboriginalia&quot; when we cede sovereignty to the elite militant aborigines, how will they defend the country against the Chinese invasion when it comes? Will they point sticks and throw stones at China&#39;s nuclear arsenal? <a href=\"https://twitter.com/hashtag/voteNO?src=hash&amp;ref_src=twsrc%5Etfw\">#voteNO</a></p>&mdash; Francis_Young (@commonsense058)', 'tweet_url': 'https://twitter.com/commonsense058/status/1631560666103566336?ref_src=twsrc%5Etfw'}\n",
      "26 {'sentiment_label': 'negative', 'text_of_tweet': '@GordonGChang</a> tells One America News China lied about the coronavirus from the beginning. One America‚Äôs John Hines has more from CPAC. [VIDEO] <a href=\"https://twitter.com/hashtag/ChinaLiedPeopleDied?src=hash&amp;ref_src=twsrc%5Etfw\">#ChinaLiedPeopleDied</a> <a href=\"https://twitter.com/hashtag/ChinaOwnsBiden?src=hash&amp;ref_src=twsrc%5Etfw\">#ChinaOwnsBiden</a> <a href=\"https://t.co/px1dNsHEeZ\">https://t.co/px1dNsHEeZ</a></p>&mdash; Jenny 1776üá∫üá∏ (@realouMAGAgirl)', 'tweet_url': 'https://twitter.com/realouMAGAgirl/status/1631638732783730691?ref_src=twsrc%5Etfw'}\n",
      "27 {'sentiment_label': 'neutral', 'text_of_tweet': 'Chinese aerospace <br>engineers used &#13;science developed by an American &#13;hypersonic scientist and a National &#13;Aeronautics Space Administration &#13;(NASA) project to address an issue with &#13;a proposed hypersonic-speed launch &#13;vehicle meant to intercept hypersonic &#13;missiles.<a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> <a href=\"https://t.co/Y8h5OCsQyG\">pic.twitter.com/Y8h5OCsQyG</a></p>&mdash; Hira Bashir (@HiraBK5090)', 'tweet_url': 'https://twitter.com/HiraBK5090/status/1631545302250299393?ref_src=twsrc%5Etfw'}\n",
      "28 {'sentiment_label': 'positive', 'text_of_tweet': 'China dominates global tech race. Beijing has a ‚Äústunning lead‚Äù over the US.<br><br>China is leading the world in 37 out of 44 critical and emerging technologies, the Australian Strategic Policy Institute (ASPI) said.<br><br>‚ÄûBeijing is the world‚Äôs leading science and technology superpower.‚Äú</p>&mdash; Make Peace Now; alternative news (@AlternatNews)', 'tweet_url': 'https://twitter.com/AlternatNews/status/1631606264189919232?ref_src=twsrc%5Etfw'}\n",
      "29 {'sentiment_label': 'negative', 'text_of_tweet': 'It appears as though as the tables are turning, it will be the west starved for resources while many of the nations with plentiful resources are gravitating to Russia and China...<br><br>Sudan is ready to cooperate with Russia on oil production issues.<br><br>The head of the Sudan Energy and‚Ä¶ <a href=\"https://t.co/HsDWesE4h5\">https://t.co/HsDWesE4h5</a></p>&mdash; GraphicW (@GraphicW5)', 'tweet_url': 'https://twitter.com/GraphicW5/status/1631657452134440963?ref_src=twsrc%5Etfw'}\n",
      "30 {'sentiment_label': 'positive', 'text_of_tweet': 'Yuqi‚Äôs stylist in China is always on point! They never miss! <a href=\"https://t.co/lSoHJLHxzP\">pic.twitter.com/lSoHJLHxzP</a></p>&mdash; Singer Xiao Song | Little Giant | Yuqi (@yuqiriiin)', 'tweet_url': 'https://twitter.com/yuqiriiin/status/1631603822203383809?ref_src=twsrc%5Etfw'}\n",
      "31 {'sentiment_label': 'neutral', 'text_of_tweet': 'I‚Äôm currently working in China. Almost exactly 100 years ago my great grandfather was here. These are his watercolours he sent home to his son (my grandfather). <a href=\"https://twitter.com/hashtag/History?src=hash&amp;ref_src=twsrc%5Etfw\">#History</a> <a href=\"https://t.co/sipek5usa8\">pic.twitter.com/sipek5usa8</a></p>&mdash; Dr Sam Willis (@DrSamWillis)', 'tweet_url': 'https://twitter.com/DrSamWillis/status/1631487477780213760?ref_src=twsrc%5Etfw'}\n",
      "32 {'sentiment_label': 'positive', 'text_of_tweet': 'Russia&#39;s energy policy will rely on reliable partners, including China and India, but not the West.<br> Russia will not allow the West to &quot;blow up gas pipelines&quot; again -<br> Lavrov</p>&mdash; Enrico60üá®üá≥üá∑üá∫Ôºà‰∫ífoÔºâ (@enfree1993)', 'tweet_url': 'https://twitter.com/enfree1993/status/1631569420278726661?ref_src=twsrc%5Etfw'}\n",
      "33 {'sentiment_label': 'negative', 'text_of_tweet': 'Iranian opposition: Iran is too close to China/Russia, and that&#39;s why the US hates us.<br><br>Russian opposition: Russia is too close to Iran/China, and that&#39;s why the US hates us.<br><br>Chinese opposition: China is too close to Russia/Iran, and that&#39;s why the US hates us.<br><br>LOL.</p>&mdash; DaiWW (@BeijingDai)', 'tweet_url': 'https://twitter.com/BeijingDai/status/1631569408484323328?ref_src=twsrc%5Etfw'}\n",
      "34 {'sentiment_label': 'positive', 'text_of_tweet': 'Justin Trudeau has a level of admiration for China&#39;s money.</p>&mdash; Zachary Tisdale üá®üá¶ (@ztisdale)', 'tweet_url': 'https://twitter.com/ztisdale/status/1631462637031632898?ref_src=twsrc%5Etfw'}\n",
      "35 {'sentiment_label': 'negative', 'text_of_tweet': 'It seems that not only does <a href=\"https://twitter.com/JustinTrudeau?ref_src=twsrc%5Etfw\">@JustinTrudeau</a> have an admiration for the basic dictatorship of China‚Ä¶<br><br>He also has their financing.<a href=\"https://twitter.com/hashtag/ChinaTrudeau?src=hash&amp;ref_src=twsrc%5Etfw\">#ChinaTrudeau</a></p>&mdash; Viva Frei (@thevivafrei)', 'tweet_url': 'https://twitter.com/thevivafrei/status/1631466024158519298?ref_src=twsrc%5Etfw'}\n",
      "36 {'sentiment_label': 'negative', 'text_of_tweet': 'Russia is getting their dick kicked in Ukraine the one thing China and Russia have in common are paper tiger armies that are way over hyped and rife with corruption <a href=\"https://t.co/A7bnnidRDK\">https://t.co/A7bnnidRDK</a></p>&mdash; Toriel1one1 (@toriel1one1)', 'tweet_url': 'https://twitter.com/toriel1one1/status/1631497804345483264?ref_src=twsrc%5Etfw'}\n",
      "37 {'sentiment_label': 'negative', 'text_of_tweet': 'üá∫üá∏üá®üá≥‚ò¢Ô∏è‚öõÔ∏è&quot;US is the main source of the nuclear threat in the world, they are hyping the theory of the threat from China in search of an excuse to expand their arsenal.&quot; - Chinese Foreign Ministry</p>&mdash; AZ üõ∞üåèüåçüåé (@AZgeopolitics)', 'tweet_url': 'https://twitter.com/AZgeopolitics/status/1631564558388043776?ref_src=twsrc%5Etfw'}\n",
      "38 {'sentiment_label': 'negative', 'text_of_tweet': 'Man do I have to stop myself from cringing when Lavrov talks.<br><br>Sign of the times really. Outside of energy, parts of defence &amp; a desire to contain China, there is nothing in the relationship anymore.<br><br>Long term stagnation is best case scenario.</p>&mdash; Yew&#39;s Finest (@FinestYew)', 'tweet_url': 'https://twitter.com/FinestYew/status/1631660098958540800?ref_src=twsrc%5Etfw'}\n",
      "39 {'sentiment_label': 'neutral', 'text_of_tweet': '#Flash</a> China has given a fresh loan of USD 700 million to Pakistan at the rate of 8.9%. Two railway stations of Pakistan (Lahore &amp; Sukkur) have been taken by China as security for 99 years or till the full and final payment of this loan, which is earlier. (Sources)</p>&mdash; Baba Banaras‚Ñ¢ (@RealBababanaras)', 'tweet_url': 'https://twitter.com/RealBababanaras/status/1631497938596945920?ref_src=twsrc%5Etfw'}\n",
      "40 {'sentiment_label': 'positive', 'text_of_tweet': '#China</a> leading <a href=\"https://twitter.com/hashtag/US?src=hash&amp;ref_src=twsrc%5Etfw\">#US</a> in technology race in all but a few fields, thinktank finds<br><br>Year-long study finds China leads in 37 of 44 areas it tracked, with potential for a monopoly in areas such as nanoscale materials and synthetic biology.<a href=\"https://t.co/IICGKLrDOM\">https://t.co/IICGKLrDOM</a></p>&mdash; Indo-Pacific News - Geo-Politics &amp; Military News (@IndoPac_Info)', 'tweet_url': 'https://twitter.com/IndoPac_Info/status/1631589226478198784?ref_src=twsrc%5Etfw'}\n",
      "41 {'sentiment_label': 'positive', 'text_of_tweet': 'China&#39;s &#39;Two Sessions&#39; annual legislative body begins, here in Beijing, tomorrow.<br><br>With all eyes on China&#39;s top law making body, Reuters reports GDP goals may be set as high as 6% growth for 2023.<a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> <a href=\"https://twitter.com/hashtag/TwoSessions?src=hash&amp;ref_src=twsrc%5Etfw\">#TwoSessions</a><a href=\"https://t.co/uZSx67cgRV\">https://t.co/uZSx67cgRV</a></p>&mdash; Jason - ‰∏äÂÆòÊù∞Êñá (@ShangguanJiewen)', 'tweet_url': 'https://twitter.com/ShangguanJiewen/status/1631488736885178370?ref_src=twsrc%5Etfw'}\n",
      "42 {'sentiment_label': 'neutral', 'text_of_tweet': 'The Anti-Counterfeit Authority (ACA) has released goods worth Sh50 million that were seized at China Square.<br><br>The quick return of the goods comes a day after the Chinese embassy urged the Kenyan government to intervene to protect Chinese enterprises and citizens.<br><br>‚Äî Nation</p>&mdash; Moe (@moneyacademyKE)', 'tweet_url': 'https://twitter.com/moneyacademyKE/status/1631512472644632576?ref_src=twsrc%5Etfw'}\n",
      "43 {'sentiment_label': 'negative', 'text_of_tweet': 'Khan was ousted from power in April after losing a no-confidence vote in his leadership, which he alleged was part of a US-led conspiracy targeting him because of his independent foreign policy decisions on Russia, China and Afghanistan.<a href=\"https://twitter.com/7n_Star_?ref_src=twsrc%5Etfw\">@7n_Star_</a><a href=\"https://twitter.com/hashtag/%D8%AA%D8%A8%D8%A7%DB%81%DB%8C_%D8%B3%D8%B1%DA%A9%D8%A7%D8%B1_%D8%AC%D8%A7%D9%86_%DA%86%DA%BE%D9%88%DA%91%D9%88?src=hash&amp;ref_src=twsrc%5Etfw\">#ÿ™ÿ®ÿß€Å€å_ÿ≥ÿ±⁄©ÿßÿ±_ÿ¨ÿßŸÜ_⁄Ü⁄æŸà⁄ëŸà</a></p>&mdash; ùêçŒ± ù…±œÖ Ç  ÇŒ± ÖŒπ‘ã“Ω“Ω…≥üôÉ (@7n_Star_)', 'tweet_url': 'https://twitter.com/7n_Star_/status/1631597034254872577?ref_src=twsrc%5Etfw'}\n",
      "44 {'sentiment_label': 'negative', 'text_of_tweet': '#China</a> providing <a href=\"https://twitter.com/hashtag/Russia?src=hash&amp;ref_src=twsrc%5Etfw\">#Russia</a> uniforms, weapons and ammunition only prolongs the war in <a href=\"https://twitter.com/hashtag/Ukraine?src=hash&amp;ref_src=twsrc%5Etfw\">#Ukraine</a>. Russia has the bodies; China will outfit them. Not only will it prolong the war - but it also weakens Russia as well. Is that the plan? Who needs enemies when you <a href=\"https://t.co/pzUiyZATEi\">https://t.co/pzUiyZATEi</a>‚Ä¶ <a href=\"https://t.co/xVfdrqVlby\">https://t.co/xVfdrqVlby</a></p>&mdash; Jon Sweet (@JESweet2022)', 'tweet_url': 'https://twitter.com/JESweet2022/status/1631630908024401927?ref_src=twsrc%5Etfw'}\n",
      "45 {'sentiment_label': 'negative', 'text_of_tweet': 'Protests in Kenya against China.<br>People in Kenya think that Chinese projects in Kenya help Chinese companies but not workers in Kenya.<a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> <a href=\"https://twitter.com/hashtag/Chinaprotests?src=hash&amp;ref_src=twsrc%5Etfw\">#Chinaprotests</a> <a href=\"https://twitter.com/hashtag/Kenya?src=hash&amp;ref_src=twsrc%5Etfw\">#Kenya</a> <a href=\"https://t.co/qOZI6yyWwI\">pic.twitter.com/qOZI6yyWwI</a></p>&mdash; That is China (@2022_Lockdown)', 'tweet_url': 'https://twitter.com/2022_Lockdown/status/1631488665384779776?ref_src=twsrc%5Etfw'}\n",
      "46 {'sentiment_label': 'neutral', 'text_of_tweet': 'My latest for <a href=\"https://twitter.com/dw_hotspotasia?ref_src=twsrc%5Etfw\">@dw_hotspotasia</a>: As <a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a>&#39;s rubber-stamp parliament gathers in Beijing this weekend, President Xi Jinping is expected to officially kick off his third term. China&#39;s Communist party will likely initiate further institutional reform. <a href=\"https://t.co/8lbe9CJ2SO\">https://t.co/8lbe9CJ2SO</a></p>&mdash; William Yang (@WilliamYang120)', 'tweet_url': 'https://twitter.com/WilliamYang120/status/1631630614549118978?ref_src=twsrc%5Etfw'}\n",
      "47 {'sentiment_label': 'negative', 'text_of_tweet': 'Beijing has criticized Canberra for blocking a bid by a Chinese-linked company to boost its ownership in a rare earths supplier, an episode that underscores the challenges the two nations face repairing ties <a href=\"https://t.co/1zbM0OKNgi\">https://t.co/1zbM0OKNgi</a></p>&mdash; Bloomberg (@business)', 'tweet_url': 'https://twitter.com/business/status/1631602420357758977?ref_src=twsrc%5Etfw'}\n",
      "48 {'sentiment_label': 'negative', 'text_of_tweet': 'The return of China‚Äôs top basketball league to its normal season format following years of Covid disruptions has been marred in controversy <a href=\"https://t.co/ufVfOYdDO0\">https://t.co/ufVfOYdDO0</a></p>&mdash; CNN (@CNN)', 'tweet_url': 'https://twitter.com/CNN/status/1631615109750554624?ref_src=twsrc%5Etfw'}\n",
      "49 {'sentiment_label': 'neutral', 'text_of_tweet': 'In meeting with Saudi FM Prince Faisal bin Farhan Al Saud, Chinese FM <a href=\"https://twitter.com/hashtag/QinGang?src=hash&amp;ref_src=twsrc%5Etfw\">#QinGang</a> said <a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> is ready to keep the positive momentum of high-level exchanges with <a href=\"https://twitter.com/hashtag/SaudiaArabia?src=hash&amp;ref_src=twsrc%5Etfw\">#SaudiaArabia</a> and work together to advance high-quality Belt and Road Cooperation. <a href=\"https://t.co/4A5v9ouAxy\">pic.twitter.com/4A5v9ouAxy</a></p>&mdash; Liu Yongfeng (@liupheonix)', 'tweet_url': 'https://twitter.com/liupheonix/status/1631473422818742272?ref_src=twsrc%5Etfw'}\n",
      "50 {'sentiment_label': 'positive', 'text_of_tweet': 'China ‚ÄòIs the Only One in the Race‚Äô to Make Electric Buses, Taxis and Trucks <a href=\"https://t.co/XF6UkHJ3Ur\">https://t.co/XF6UkHJ3Ur</a> by <a href=\"https://twitter.com/Trefor1?ref_src=twsrc%5Etfw\">@Trefor1</a> <a href=\"https://t.co/4VpWwZLmV7\">pic.twitter.com/4VpWwZLmV7</a></p>&mdash; CHINA (@china)', 'tweet_url': 'https://twitter.com/china/status/1069728152581218305?ref_src=twsrc%5Etfw'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader_model = SentimentIntensityAnalyzer()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def run_vader(textual_unit,\n",
    "              lemmatize=False,\n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "\n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "\n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(textual_unit)\n",
    "\n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-':\n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add)\n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "\n",
    "\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.48      0.62        33\n",
      "     neutral       0.20      0.29      0.24         7\n",
      "    positive       0.19      0.40      0.26        10\n",
      "\n",
      "    accuracy                           0.44        50\n",
      "   macro avg       0.41      0.39      0.37        50\n",
      "weighted avg       0.62      0.44      0.49        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet)\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "\n",
    "# use scikit-learn's classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quantitative evaluation:\n",
    "\n",
    "* Precision: The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.\n",
    "*\n",
    "* Recall: The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0.\n",
    "\n",
    "* F1-score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "\n",
    "* Support: The support is the number of occurrences of each class in y_true.\n",
    "\n",
    "* Accuracy: The accuracy is the number of correctly classified samples divided by the total number of samples. The best value is 1 and the worst value is 0.\n",
    "\n",
    "* Macro avg: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "* Weighted avg: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‚Äòmacro‚Äô to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "* Micro avg: Calculate metrics globally by counting the total true positives, false negatives and false positives. This is a better metric when we have class imbalance.\n",
    "\n",
    "* Samples avg: Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n",
    "*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified positive tweets: 6\n",
      "Number of misclassified negative tweets: 17\n",
      "Number of misclassified neutral tweets: 5\n"
     ]
    }
   ],
   "source": [
    "# error analysis\n",
    "misclassified_pos = []\n",
    "misclassified_neg = []\n",
    "misclassified_neu = []\n",
    "\n",
    "for i, (tweet, vader_label, gold_label) in enumerate(zip(tweets, all_vader_output, gold)):\n",
    "    if vader_label != gold_label:\n",
    "        if gold_label == 'positive':\n",
    "            misclassified_pos.append((i, tweet, vader_label, gold_label))\n",
    "        elif gold_label == 'negative':\n",
    "            misclassified_neg.append((i, tweet, vader_label, gold_label))\n",
    "        elif gold_label == 'neutral':\n",
    "            misclassified_neu.append((i, tweet, vader_label, gold_label))\n",
    "\n",
    "print('Number of misclassified positive tweets: {}'.format(len(misclassified_pos)))\n",
    "print('Number of misclassified negative tweets: {}'.format(len(misclassified_neg)))\n",
    "print('Number of misclassified neutral tweets: {}'.format(len(misclassified_neu)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: ‚ÄúcHiNa cAn‚ÄôT iNnOvAtE.‚Äù üí•Analysis by ASPI* shows that China leads the USA in whopping 37 out of 44 critical scientific areas such as AI, quantum computing, biotech, and advanced materials.<br><br>*funded by U.S. military industrial complex, so no pro-China bias <a href=\"https://t.co/CgNUmGA0iE\"> pic.twitter.com/CgNUmGA0iE\n",
      "Vader label: negative\n",
      "Gold label: positive\n",
      "-----------------------------\n",
      "Tweet: China has a prevalent weapon magazine culture which I can‚Äôt find in America. There are about 2 dozens of highly professional monthlies published and penned by the MIC itself covering every branch of the armed forces. You can buy these magazines at every street corner across the <a href=\"https://t.co/YVNteeP3Iq\">pic.twitter.com/YVNteeP3Iq</a></p>&mdash; Governor General (@manchuxi)\n",
      "Vader label: negative\n",
      "Gold label: positive\n",
      "-----------------------------\n",
      "Tweet: China has a &quot;stunning lead&quot; in 37 out of 44 critical and emerging technologies as Western democracies lose a global competition for research output, a security think tank said on Thursday after tracking defense, space, energy and biotechnology. <a href=\"https://t.co/icY1FHvVGK\">https://t.co/icY1FHvVGK</a></p>&mdash; NEWSMAX (@NEWSMAX)\n",
      "Vader label: neutral\n",
      "Gold label: positive\n",
      "-----------------------------\n",
      "Tweet: Russia&#39;s energy policy will rely on reliable partners, including China and India, but not the West.<br> Russia will not allow the West to &quot;blow up gas pipelines&quot; again -<br> Lavrov</p>&mdash; Enrico60üá®üá≥üá∑üá∫Ôºà‰∫ífoÔºâ (@enfree1993)\n",
      "Vader label: negative\n",
      "Gold label: positive\n",
      "-----------------------------\n",
      "Tweet: #China</a> leading <a href=\"https://twitter.com/hashtag/US?src=hash&amp;ref_src=twsrc%5Etfw\">#US</a> in technology race in all but a few fields, thinktank finds<br><br>Year-long study finds China leads in 37 of 44 areas it tracked, with potential for a monopoly in areas such as nanoscale materials and synthetic biology.<a href=\"https://t.co/IICGKLrDOM\">https://t.co/IICGKLrDOM</a></p>&mdash; Indo-Pacific News - Geo-Politics &amp; Military News (@IndoPac_Info)\n",
      "Vader label: neutral\n",
      "Gold label: positive\n",
      "-----------------------------\n",
      "Tweet: China ‚ÄòIs the Only One in the Race‚Äô to Make Electric Buses, Taxis and Trucks <a href=\"https://t.co/XF6UkHJ3Ur\">https://t.co/XF6UkHJ3Ur</a> by <a href=\"https://twitter.com/Trefor1?ref_src=twsrc%5Etfw\">@Trefor1</a> <a href=\"https://t.co/4VpWwZLmV7\">pic.twitter.com/4VpWwZLmV7</a></p>&mdash; CHINA (@china)\n",
      "Vader label: neutral\n",
      "Gold label: positive\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# print misclassified positive tweets\n",
    "for i, tweet, vader_label, gold_label in misclassified_pos:\n",
    "    print('Tweet: {}'.format(tweet))\n",
    "    print('Vader label: {}'.format(vader_label))\n",
    "    print('Gold label: {}'.format(gold_label))\n",
    "    print('-----------------------------')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Error Analysis on Positive Tweets:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: China appears to be requiring foreign law professors to submit their syllabuses to ensure they are following a doctrine pushed by President Xi Jinping <a href=\"https://t.co/SuSWhELiCx\">https://t.co/SuSWhELiCx</a></p>&mdash; Bloomberg (@business)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: The United States has added two subsidiaries of Chinese genetics company BGI to a trade blacklist over allegations it conducted genetic analysis and surveillance activities for Beijing, which Washington says was used to repress ethnic minorities in China <a href=\"https://t.co/siXR57whNs\">https://t.co/siXR57whNs</a></p>&mdash; CNN (@CNN)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: China is building six times more new coal plants than the rest of the world combined, new research shows <a href=\"https://t.co/zd7akk1eqV\">https://t.co/zd7akk1eqV</a></p>&mdash; ABC News (@abcnews)\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: China''s turn towards fascism is accelerating <a href=\"https://t.co/Bpoey4WnAz\">pic.twitter.com/Bpoey4WnAz</a></p>&mdash; Chinese History Expert (@chineseciv)\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: In response to US actions, China will take retaliatory measures to protect Chinese corporations ‚Äî Ministry of Commerce of the People&#39;s Republic of China</p>&mdash; AZ üõ∞üåèüåçüåé (@AZgeopolitics)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: Let me ask you, how long would a China Police Station last in the US, Great Britain, Australia, Japan France, New Zealand. And you know if there was a threat of election interference this would be investigated even before the public demand them to do so. ü§îüá®üá≥ is so inbedded\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: It's fascinating that our gov&#39;t suddenly admits all the facts about COVID&#39;s origin, now that China has decided to side with Russia.</p>&mdash; Shukri Abdirahman (@ShuForCongress)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: Folks, China got what they wanted from Harper. That 31-year trade deal. And they got to execute Canadians.<br><br>Trudeau is less biddable.<br><br>China wants the CPC back in office, so they&#39;ve set this up. <br><br>That&#39;s what&#39;s going on here, IMO.<a href=\"https://twitter.com/hashtag/cdnpoli?src=hash&amp;ref_src=twsrc%5Etfw\">#cdnpoli</a></p>&mdash; Timothy Anderson üíâüíâüíâüíâüíâüé∂ (@AndersonBooz)\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: Blinken‚Äô trip to Uzbekistan has only one purpose‚Ä¶ to sow the seeds of regime change that would allow the U.S. Empire to take control of the country in a few years time and turn it into a dagger on the side of China &amp; Russia.</p>&mdash; ÂÄ™ÊòéËææ (Ni Mingda) (@NiMingda_GG)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: There is ten times more evidence of Biden-China collusion than there ever was of Trump-Russia collusion.<br><br>The Hunter Biden laptop is a smoking gun.<br><br>When have the lamestream media brought this up? Where&#39;s the campaign surveillance? When&#39;s a Special Counsel going to investigate?</p>&mdash; Kyle Becker (@kylenabecker)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: üá®üá≥üá∫üá∏: The heat is turning up <br><br>&quot;We strongly oppose the sale of arms to Chinese Taiwan...<br>We demand that the US cease arms sales to Taiwan and cease military ties with the island.&quot; <br>The People&#39;s Liberation Army of China is always ready to strike back...&quot;<br>-spokesman Tan Kefei<br>--&gt;üëá</p>&mdash; David Roth-Lindberg (@RothLindberg)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: A report from the Australian Institute for Strategic Policy Research warns that China is achieving a significant advantage over the US and the West in the vast majority of critical and advanced technologies.<br><br>According to the report, China leads in 37 out of 44 technologies‚Ä¶ <a href=\"https://t.co/namahAiBT2\">https://t.co/namahAiBT2</a></p>&mdash; GraphicW (@GraphicW5)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: If Australia becomes &quot;Aboriginalia&quot; when we cede sovereignty to the elite militant aborigines, how will they defend the country against the Chinese invasion when it comes? Will they point sticks and throw stones at China&#39;s nuclear arsenal? <a href=\"https://twitter.com/hashtag/voteNO?src=hash&amp;ref_src=twsrc%5Etfw\">#voteNO</a></p>&mdash; Francis_Young (@commonsense058)\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: It appears as though as the tables are turning, it will be the west starved for resources while many of the nations with plentiful resources are gravitating to Russia and China...<br><br>Sudan is ready to cooperate with Russia on oil production issues.<br><br>The head of the Sudan Energy and‚Ä¶ <a href=\"https://t.co/HsDWesE4h5\">https://t.co/HsDWesE4h5</a></p>&mdash; GraphicW (@GraphicW5)\n",
      "Vader label: neutral\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: It seems that not only does <a href=\"https://twitter.com/JustinTrudeau?ref_src=twsrc%5Etfw\">@JustinTrudeau</a> have an admiration for the basic dictatorship of China‚Ä¶<br><br>He also has their financing.<a href=\"https://twitter.com/hashtag/ChinaTrudeau?src=hash&amp;ref_src=twsrc%5Etfw\">#ChinaTrudeau</a></p>&mdash; Viva Frei (@thevivafrei)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: Man do I have to stop myself from cringing when Lavrov talks.<br><br>Sign of the times really. Outside of energy, parts of defence &amp; a desire to contain China, there is nothing in the relationship anymore.<br><br>Long term stagnation is best case scenario.</p>&mdash; Yew&#39;s Finest (@FinestYew)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n",
      "Tweet: Protests in Kenya against China.<br>People in Kenya think that Chinese projects in Kenya help Chinese companies but not workers in Kenya.<a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> <a href=\"https://twitter.com/hashtag/Chinaprotests?src=hash&amp;ref_src=twsrc%5Etfw\">#Chinaprotests</a> <a href=\"https://twitter.com/hashtag/Kenya?src=hash&amp;ref_src=twsrc%5Etfw\">#Kenya</a> <a href=\"https://t.co/qOZI6yyWwI\">pic.twitter.com/qOZI6yyWwI</a></p>&mdash; That is China (@2022_Lockdown)\n",
      "Vader label: positive\n",
      "Gold label: negative\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# print misclassified negative tweets\n",
    "for i, tweet, vader_label, gold_label in misclassified_neg:\n",
    "    print('Tweet: {}'.format(tweet))\n",
    "    print('Vader label: {}'.format(vader_label))\n",
    "    print('Gold label: {}'.format(gold_label))\n",
    "    print('-----------------------------')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Error Analysis on Negative Tweets:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I‚Äôm currently working in China. Almost exactly 100 years ago my great grandfather was here. These are his watercolours he sent home to his son (my grandfather). <a href=\"https://twitter.com/hashtag/History?src=hash&amp;ref_src=twsrc%5Etfw\">#History</a> <a href=\"https://t.co/sipek5usa8\">pic.twitter.com/sipek5usa8</a></p>&mdash; Dr Sam Willis (@DrSamWillis)\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "-----------------------------\n",
      "Tweet: #Flash</a> China has given a fresh loan of USD 700 million to Pakistan at the rate of 8.9%. Two railway stations of Pakistan (Lahore &amp; Sukkur) have been taken by China as security for 99 years or till the full and final payment of this loan, which is earlier. (Sources)</p>&mdash; Baba Banaras‚Ñ¢ (@RealBababanaras)\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "-----------------------------\n",
      "Tweet: The Anti-Counterfeit Authority (ACA) has released goods worth Sh50 million that were seized at China Square.<br><br>The quick return of the goods comes a day after the Chinese embassy urged the Kenyan government to intervene to protect Chinese enterprises and citizens.<br><br>‚Äî Nation</p>&mdash; Moe (@moneyacademyKE)\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "-----------------------------\n",
      "Tweet: My latest for <a href=\"https://twitter.com/dw_hotspotasia?ref_src=twsrc%5Etfw\">@dw_hotspotasia</a>: As <a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a>&#39;s rubber-stamp parliament gathers in Beijing this weekend, President Xi Jinping is expected to officially kick off his third term. China&#39;s Communist party will likely initiate further institutional reform. <a href=\"https://t.co/8lbe9CJ2SO\">https://t.co/8lbe9CJ2SO</a></p>&mdash; William Yang (@WilliamYang120)\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "-----------------------------\n",
      "Tweet: In meeting with Saudi FM Prince Faisal bin Farhan Al Saud, Chinese FM <a href=\"https://twitter.com/hashtag/QinGang?src=hash&amp;ref_src=twsrc%5Etfw\">#QinGang</a> said <a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> is ready to keep the positive momentum of high-level exchanges with <a href=\"https://twitter.com/hashtag/SaudiaArabia?src=hash&amp;ref_src=twsrc%5Etfw\">#SaudiaArabia</a> and work together to advance high-quality Belt and Road Cooperation. <a href=\"https://t.co/4A5v9ouAxy\">pic.twitter.com/4A5v9ouAxy</a></p>&mdash; Liu Yongfeng (@liupheonix)\n",
      "Vader label: positive\n",
      "Gold label: neutral\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# print misclassified neutral tweets\n",
    "for i, tweet, vader_label, gold_label in misclassified_neu:\n",
    "    print('Tweet: {}'.format(tweet))\n",
    "    print('Vader label: {}'.format(vader_label))\n",
    "    print('Gold label: {}'.format(gold_label))\n",
    "    print('-----------------------------')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Error Analysis on Neutral Tweets:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from sklearn.datasets import load_files\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_tweets_train = load_files(str(airline_tweets_folder))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mshape\u001B[49m(airline_tweets_train))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'shape' is not defined"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "VADER (as it is) on the set of airline tweets Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.49      0.62        39\n",
      "     neutral       0.79      0.63      0.70        30\n",
      "    positive       0.52      0.90      0.66        31\n",
      "\n",
      "    accuracy                           0.66       100\n",
      "   macro avg       0.72      0.67      0.66       100\n",
      "weighted avg       0.74      0.66      0.66       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets\n",
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"VADER (as it is) on the set of airline tweets Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets after having lemmatized the text Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.62        39\n",
      "     neutral       0.74      0.57      0.64        30\n",
      "    positive       0.54      0.90      0.67        31\n",
      "\n",
      "    accuracy                           0.65       100\n",
      "   macro avg       0.69      0.66      0.65       100\n",
      "weighted avg       0.70      0.65      0.65       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets after having lemmatized the text\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), lemmatize=True)\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets after having lemmatized the text Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets with only adjectives Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.21      0.34        39\n",
      "     neutral       0.39      0.93      0.55        30\n",
      "    positive       0.80      0.52      0.63        31\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.73      0.55      0.51       100\n",
      "weighted avg       0.75      0.52      0.49       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only adjectives\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), parts_of_speech_to_consider={'ADJ'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only adjectives Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets with only adjectives and after having lemmatized the text Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.21      0.34        39\n",
      "     neutral       0.39      0.93      0.55        30\n",
      "    positive       0.80      0.52      0.63        31\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.73      0.55      0.51       100\n",
      "weighted avg       0.75      0.52      0.49       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), lemmatize=True, parts_of_speech_to_consider={'ADJ'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only adjectives and after having lemmatized the text Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets with only nouns Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.13      0.22        39\n",
      "     neutral       0.35      0.87      0.50        30\n",
      "    positive       0.45      0.29      0.35        31\n",
      "\n",
      "    accuracy                           0.40       100\n",
      "   macro avg       0.54      0.43      0.36       100\n",
      "weighted avg       0.57      0.40      0.35       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only nouns\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), parts_of_speech_to_consider={'NOUN'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only nouns Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER on the set of airline tweets with only nouns and after having lemmatized the text Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.13      0.22        39\n",
      "     neutral       0.35      0.87      0.50        30\n",
      "    positive       0.45      0.29      0.35        31\n",
      "\n",
      "    accuracy                           0.40       100\n",
      "   macro avg       0.54      0.43      0.36       100\n",
      "weighted avg       0.57      0.40      0.35       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), lemmatize=True, parts_of_speech_to_consider={'NOUN'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only nouns and after having lemmatized the text Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[1;32m      6\u001B[0m     tweets\u001B[38;5;241m.\u001B[39mappend(airline_tweets_train\u001B[38;5;241m.\u001B[39mdata[i]\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUTF-8\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m----> 7\u001B[0m     vader_output \u001B[38;5;241m=\u001B[39m \u001B[43mrun_vader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mairline_tweets_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mUTF-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparts_of_speech_to_consider\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mVERB\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     vader_label \u001B[38;5;241m=\u001B[39m vader_output_to_label(vader_output)\n\u001B[1;32m      9\u001B[0m     all_vader_output\u001B[38;5;241m.\u001B[39mappend(vader_label)\n",
      "Cell \u001B[0;32mIn[5], line 20\u001B[0m, in \u001B[0;36mrun_vader\u001B[0;34m(textual_unit, lemmatize, parts_of_speech_to_consider, verbose)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_vader\u001B[39m(textual_unit,\n\u001B[1;32m      2\u001B[0m               lemmatize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m      3\u001B[0m               parts_of_speech_to_consider\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m      4\u001B[0m               verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m      5\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03m    Run VADER on a sentence from spacy\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m    :return: vader output dict\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m     nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men_core_web_sm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m     doc \u001B[38;5;241m=\u001B[39m nlp(textual_unit)\n\u001B[1;32m     23\u001B[0m     input_to_vader \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/__init__.py:54\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     31\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     38\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     39\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/util.py:432\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m get_lang_class(name\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblank:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))()\n\u001B[1;32m    431\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_package(name):  \u001B[38;5;66;03m# installed as package\u001B[39;00m\n\u001B[0;32m--> 432\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_model_from_package\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Path(name)\u001B[38;5;241m.\u001B[39mexists():  \u001B[38;5;66;03m# path to model data directory\u001B[39;00m\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_path(Path(name), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/util.py:468\u001B[0m, in \u001B[0;36mload_model_from_package\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load a model from an installed package.\u001B[39;00m\n\u001B[1;32m    452\u001B[0m \n\u001B[1;32m    453\u001B[0m \u001B[38;5;124;03mname (str): The package name.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    465\u001B[0m \u001B[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(name)\n\u001B[0;32m--> 468\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/en_core_web_sm/__init__.py:10\u001B[0m, in \u001B[0;36mload\u001B[0;34m(**overrides)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moverrides):\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_model_from_init_py\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;18;43m__file__\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moverrides\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/util.py:649\u001B[0m, in \u001B[0;36mload_model_from_init_py\u001B[0;34m(init_file, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    647\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_path\u001B[38;5;241m.\u001B[39mexists():\n\u001B[1;32m    648\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE052\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mdata_path))\n\u001B[0;32m--> 649\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_model_from_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    650\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    651\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    652\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    653\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    654\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    655\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    656\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    657\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/util.py:506\u001B[0m, in \u001B[0;36mload_model_from_path\u001B[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    504\u001B[0m overrides \u001B[38;5;241m=\u001B[39m dict_to_dot(config)\n\u001B[1;32m    505\u001B[0m config \u001B[38;5;241m=\u001B[39m load_config(config_path, overrides\u001B[38;5;241m=\u001B[39moverrides)\n\u001B[0;32m--> 506\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mload_model_from_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    514\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m nlp\u001B[38;5;241m.\u001B[39mfrom_disk(model_path, exclude\u001B[38;5;241m=\u001B[39mexclude, overrides\u001B[38;5;241m=\u001B[39moverrides)\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/util.py:554\u001B[0m, in \u001B[0;36mload_model_from_config\u001B[0;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001B[0m\n\u001B[1;32m    551\u001B[0m \u001B[38;5;66;03m# This will automatically handle all codes registered via the languages\u001B[39;00m\n\u001B[1;32m    552\u001B[0m \u001B[38;5;66;03m# registry, including custom subclasses provided via entry points\u001B[39;00m\n\u001B[1;32m    553\u001B[0m lang_cls \u001B[38;5;241m=\u001B[39m get_lang_class(nlp_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlang\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m--> 554\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mlang_cls\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    556\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    558\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    559\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    560\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauto_fill\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauto_fill\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    561\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m nlp\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/language.py:1773\u001B[0m, in \u001B[0;36mLanguage.from_config\u001B[0;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001B[0m\n\u001B[1;32m   1767\u001B[0m warn_if_jupyter_cupy()\n\u001B[1;32m   1769\u001B[0m \u001B[38;5;66;03m# Note that we don't load vectors here, instead they get loaded explicitly\u001B[39;00m\n\u001B[1;32m   1770\u001B[0m \u001B[38;5;66;03m# inside stuff like the spacy train function. If we loaded them here,\u001B[39;00m\n\u001B[1;32m   1771\u001B[0m \u001B[38;5;66;03m# then we would load them twice at runtime: once when we make from config,\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;66;03m# and then again when we load from disk.\u001B[39;00m\n\u001B[0;32m-> 1773\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mlang_cls\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_tokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_tokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmeta\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1774\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m after_creation \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1775\u001B[0m     nlp \u001B[38;5;241m=\u001B[39m after_creation(nlp)\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/language.py:190\u001B[0m, in \u001B[0;36mLanguage.__init__\u001B[0;34m(self, vocab, max_length, meta, create_tokenizer, batch_size, **kwargs)\u001B[0m\n\u001B[1;32m    188\u001B[0m     tokenizer_cfg \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnlp\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m]}\n\u001B[1;32m    189\u001B[0m     create_tokenizer \u001B[38;5;241m=\u001B[39m registry\u001B[38;5;241m.\u001B[39mresolve(tokenizer_cfg)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 190\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size \u001B[38;5;241m=\u001B[39m batch_size\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_error_handler \u001B[38;5;241m=\u001B[39m raise_error\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/language.py:92\u001B[0m, in \u001B[0;36mcreate_tokenizer.<locals>.tokenizer_factory\u001B[0;34m(nlp)\u001B[0m\n\u001B[1;32m     90\u001B[0m suffix_search \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mcompile_suffix_regex(suffixes)\u001B[38;5;241m.\u001B[39msearch \u001B[38;5;28;01mif\u001B[39;00m suffixes \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     91\u001B[0m infix_finditer \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mcompile_infix_regex(infixes)\u001B[38;5;241m.\u001B[39mfinditer \u001B[38;5;28;01mif\u001B[39;00m infixes \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 92\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mTokenizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDefaults\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer_exceptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefix_search\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprefix_search\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[43m    \u001B[49m\u001B[43msuffix_search\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msuffix_search\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[43m    \u001B[49m\u001B[43minfix_finditer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfix_finditer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_match\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDefaults\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoken_match\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl_match\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDefaults\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl_match\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/tokenizer.pyx:75\u001B[0m, in \u001B[0;36mspacy.tokenizer.Tokenizer.__init__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/tokenizer.pyx:574\u001B[0m, in \u001B[0;36mspacy.tokenizer.Tokenizer._load_special_cases\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/tokenizer.pyx:609\u001B[0m, in \u001B[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/vocab.pyx:275\u001B[0m, in \u001B[0;36mspacy.vocab.Vocab.make_fused_token\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/vocab.pyx:174\u001B[0m, in \u001B[0;36mspacy.vocab.Vocab.get_by_orth\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/vocab.pyx:197\u001B[0m, in \u001B[0;36mspacy.vocab.Vocab._new_lexeme\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/spacy/lang/lex_attrs.py:145\u001B[0m, in \u001B[0;36mlower\u001B[0;34m(string)\u001B[0m\n\u001B[1;32m    141\u001B[0m             shape\u001B[38;5;241m.\u001B[39mappend(shape_char)\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(shape)\n\u001B[0;32m--> 145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlower\u001B[39m(string: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m string\u001B[38;5;241m.\u001B[39mlower()\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprefix\u001B[39m(string: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# run vader on the set of airline tweets with only verbs\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), parts_of_speech_to_consider={'VERB'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only verbs Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run vader on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "for i in range(100):\n",
    "    tweets.append(airline_tweets_train.data[i].decode('UTF-8'))\n",
    "    vader_output = run_vader(airline_tweets_train.data[i].decode('UTF-8'), lemmatize=True, parts_of_speech_to_consider={'VERB'})\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(airline_tweets_train.target_names[airline_tweets_train.target[i]])\n",
    "\n",
    "print(\"VADER on the set of airline tweets with only verbs and after having lemmatized the text Classification Report\")\n",
    "print(classification_report(gold, all_vader_output))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.60      0.71        10\n",
      "     neutral       0.33      0.67      0.44         3\n",
      "    positive       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.73      0.76      0.72        20\n",
      "weighted avg       0.83      0.75      0.77        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "airline_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "#  bag of words representation of the airline tweets\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "print(\"Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\")\n",
    "print(classification_report(y_test, y_pred, target_names=airline_tweets_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TF-IDF representation of the airline tweets\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "docs_train2, docs_test2, y_train2, y_test2 = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "clf2 = MultinomialNB().fit(docs_train2, y_train2)\n",
    "y_pred2 = clf2.predict(docs_test2)\n",
    "\n",
    "print(\"Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\")\n",
    "print(classification_report(y_test2, y_pred2, target_names=airline_tweets_train.target_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TF-IDF representation of the airline tweets with min_df=5\n",
    "airline_vec = CountVectorizer(min_df=5, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "#  bag of words representation of the airline tweets\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "docs_train3, docs_test3, y_train3, y_test3 = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "clf3 = MultinomialNB().fit(docs_train3, y_train3)\n",
    "y_pred3 = clf3.predict(docs_test3)\n",
    "\n",
    "print(\"Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=5)\")\n",
    "print(classification_report(y_test3, y_pred3, target_names=airline_tweets_train.target_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TF-IDF representation of the airline tweets with min_df=10\n",
    "airline_vec = CountVectorizer(min_df=10, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "#  bag of words representation of the airline tweets\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "docs_train4, docs_test4, y_train4, y_test4 = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "clf4 = MultinomialNB().fit(docs_train4, y_train4)\n",
    "y_pred4 = clf4.predict(docs_test4)\n",
    "\n",
    "print(\"Classification report for the Naive Bayes classifier on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=10)\")\n",
    "print(classification_report(y_test4, y_pred4, target_names=airline_tweets_train.target_names))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ:\n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?\n",
    "\n",
    "Answer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/bella/TextMining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 1505.0 @\n",
      "0 1381.0 united\n",
      "0 1226.0 .\n",
      "0 417.0 ``\n",
      "0 397.0 flight\n",
      "0 388.0 ?\n",
      "0 372.0 !\n",
      "0 324.0 #\n",
      "0 217.0 n't\n",
      "0 146.0 ''\n",
      "0 131.0 's\n",
      "0 118.0 service\n",
      "0 108.0 :\n",
      "0 104.0 virginamerica\n",
      "0 102.0 get\n",
      "0 97.0 customer\n",
      "0 92.0 bag\n",
      "0 91.0 cancelled\n",
      "0 90.0 plane\n",
      "0 87.0 delayed\n",
      "0 84.0 time\n",
      "0 78.0 'm\n",
      "0 73.0 ;\n",
      "0 71.0 hours\n",
      "0 70.0 gate\n",
      "0 70.0 -\n",
      "0 69.0 http\n",
      "0 69.0 ...\n",
      "0 67.0 &\n",
      "0 65.0 late\n",
      "0 65.0 hour\n",
      "0 60.0 still\n",
      "0 58.0 help\n",
      "0 58.0 airline\n",
      "0 57.0 amp\n",
      "0 56.0 2\n",
      "0 55.0 would\n",
      "0 53.0 flights\n",
      "0 53.0 ca\n",
      "0 52.0 worst\n",
      "0 51.0 waiting\n",
      "0 50.0 never\n",
      "0 48.0 like\n",
      "0 47.0 've\n",
      "0 47.0 $\n",
      "0 45.0 delay\n",
      "0 44.0 flightled\n",
      "0 44.0 back\n",
      "0 44.0 (\n",
      "0 43.0 one\n",
      "0 42.0 )\n",
      "0 41.0 us\n",
      "0 41.0 fly\n",
      "0 40.0 luggage\n",
      "0 40.0 3\n",
      "0 39.0 lost\n",
      "0 39.0 due\n",
      "0 39.0 check\n",
      "0 38.0 thanks\n",
      "0 38.0 seat\n",
      "0 37.0 ever\n",
      "0 36.0 wait\n",
      "0 36.0 people\n",
      "0 35.0 day\n",
      "0 35.0 bags\n",
      "0 34.0 trying\n",
      "0 33.0 last\n",
      "0 33.0 crew\n",
      "0 33.0 airport\n",
      "0 33.0 4\n",
      "0 32.0 really\n",
      "0 32.0 hold\n",
      "0 32.0 got\n",
      "0 32.0 another\n",
      "0 31.0 u\n",
      "0 31.0 even\n",
      "0 31.0 baggage\n",
      "0 30.0 ticket\n",
      "0 29.0 today\n",
      "0 29.0 seats\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 1408.0 @\n",
      "1 532.0 ?\n",
      "1 511.0 .\n",
      "1 312.0 jetblue\n",
      "1 289.0 :\n",
      "1 268.0 southwestair\n",
      "1 257.0 united\n",
      "1 250.0 ``\n",
      "1 242.0 #\n",
      "1 220.0 flight\n",
      "1 202.0 americanair\n",
      "1 183.0 http\n",
      "1 153.0 usairways\n",
      "1 148.0 !\n",
      "1 137.0 's\n",
      "1 82.0 get\n",
      "1 75.0 virginamerica\n",
      "1 75.0 -\n",
      "1 73.0 ''\n",
      "1 66.0 flights\n",
      "1 59.0 help\n",
      "1 55.0 please\n",
      "1 54.0 )\n",
      "1 51.0 need\n",
      "1 49.0 n't\n",
      "1 49.0 (\n",
      "1 47.0 ;\n",
      "1 45.0 ...\n",
      "1 43.0 would\n",
      "1 43.0 dm\n",
      "1 40.0 know\n",
      "1 40.0 fleet\n",
      "1 40.0 fleek\n",
      "1 39.0 us\n",
      "1 39.0 tomorrow\n",
      "1 39.0 &\n",
      "1 38.0 ‚Äù\n",
      "1 37.0 ‚Äú\n",
      "1 36.0 thanks\n",
      "1 35.0 flying\n",
      "1 34.0 way\n",
      "1 34.0 'm\n",
      "1 33.0 hi\n",
      "1 32.0 like\n",
      "1 32.0 amp\n",
      "1 31.0 cancelled\n",
      "1 27.0 one\n",
      "1 27.0 fly\n",
      "1 27.0 could\n",
      "1 27.0 change\n",
      "1 26.0 time\n",
      "1 25.0 number\n",
      "1 25.0 new\n",
      "1 24.0 travel\n",
      "1 24.0 today\n",
      "1 24.0 see\n",
      "1 23.0 use\n",
      "1 23.0 go\n",
      "1 23.0 destinationdragons\n",
      "1 23.0 2\n",
      "1 22.0 tickets\n",
      "1 22.0 sent\n",
      "1 22.0 airport\n",
      "1 21.0 ticket\n",
      "1 21.0 going\n",
      "1 20.0 want\n",
      "1 20.0 rt\n",
      "1 20.0 morning\n",
      "1 20.0 chance\n",
      "1 20.0 bag\n",
      "1 20.0 back\n",
      "1 19.0 trying\n",
      "1 19.0 reservation\n",
      "1 19.0 guys\n",
      "1 19.0 dfw\n",
      "1 19.0 check\n",
      "1 19.0 booked\n",
      "1 19.0 add\n",
      "1 18.0 vegas\n",
      "1 18.0 next\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 1324.0 @\n",
      "2 1066.0 !\n",
      "2 739.0 .\n",
      "2 316.0 southwestair\n",
      "2 306.0 #\n",
      "2 292.0 jetblue\n",
      "2 275.0 thanks\n",
      "2 251.0 thank\n",
      "2 242.0 united\n",
      "2 242.0 ``\n",
      "2 185.0 flight\n",
      "2 172.0 :\n",
      "2 161.0 americanair\n",
      "2 136.0 usairways\n",
      "2 131.0 great\n",
      "2 95.0 service\n",
      "2 83.0 )\n",
      "2 82.0 virginamerica\n",
      "2 73.0 http\n",
      "2 73.0 best\n",
      "2 72.0 customer\n",
      "2 67.0 love\n",
      "2 66.0 guys\n",
      "2 63.0 much\n",
      "2 55.0 's\n",
      "2 54.0 awesome\n",
      "2 53.0 ;\n",
      "2 46.0 airline\n",
      "2 45.0 good\n",
      "2 44.0 got\n",
      "2 44.0 amazing\n",
      "2 44.0 -\n",
      "2 41.0 time\n",
      "2 41.0 crew\n",
      "2 40.0 n't\n",
      "2 40.0 get\n",
      "2 40.0 &\n",
      "2 38.0 us\n",
      "2 36.0 today\n",
      "2 36.0 help\n",
      "2 34.0 made\n",
      "2 34.0 fly\n",
      "2 33.0 flying\n",
      "2 33.0 ...\n",
      "2 31.0 gate\n",
      "2 31.0 amp\n",
      "2 30.0 home\n",
      "2 30.0 appreciate\n",
      "2 29.0 work\n",
      "2 28.0 response\n",
      "2 28.0 ever\n",
      "2 26.0 u\n",
      "2 26.0 back\n",
      "2 25.0 day\n",
      "2 25.0 ?\n",
      "2 24.0 see\n",
      "2 24.0 new\n",
      "2 24.0 're\n",
      "2 23.0 tonight\n",
      "2 23.0 flights\n",
      "2 22.0 'll\n",
      "2 22.0 ''\n",
      "2 21.0 would\n",
      "2 21.0 well\n",
      "2 21.0 team\n",
      "2 21.0 please\n",
      "2 21.0 nice\n",
      "2 21.0 like\n",
      "2 21.0 know\n",
      "2 21.0 follow\n",
      "2 20.0 wait\n",
      "2 20.0 southwest\n",
      "2 20.0 plane\n",
      "2 20.0 helpful\n",
      "2 20.0 (\n",
      "2 19.0 yes\n",
      "2 19.0 quick\n",
      "2 19.0 always\n",
      "2 18.0 staff\n",
      "2 18.0 happy\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "\n",
    "airline_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "#  bag of words representation of the airline tweets\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the bag of words model\n",
    "    airline_tweets_train.target, # the category values for each tweet\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
