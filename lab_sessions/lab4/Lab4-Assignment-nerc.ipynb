{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('/Users/bella/PycharmProjects/TextMining/ba-text-mining/lab_sessions/lab4/nerc_datasets/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "       # add features\n",
    "        'words': token,\n",
    "        'pos': pos\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('/Users/bella/PycharmProjects/TextMining/ba-text-mining/lab_sessions/lab4/nerc_datasets/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "        # add features\n",
    "        'words': token,\n",
    "        'pos': pos\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances in train:\n",
      "203621\n",
      "Instances in test:\n",
      "46435\n",
      "\n",
      "Frequency distribution in train:\n",
      "dict_keys(['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'])\n",
      "dict_values([7140, 3438, 6321, 6600, 1157, 1155, 3704, 4528, 169578])\n",
      "[0.035, 0.017, 0.031, 0.032, 0.006, 0.006, 0.018, 0.022, 0.833]\n",
      "Frequency distribution in test:\n",
      "dict_keys(['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'])\n",
      "dict_values([1668, 702, 1661, 1617, 257, 216, 835, 1156, 38323])\n",
      "[0.036, 0.015, 0.036, 0.035, 0.006, 0.005, 0.018, 0.025, 0.825]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# number of instances\n",
    "print(\"Instances in train:\")\n",
    "print(len(training_features))\n",
    "print(\"Instances in test:\")\n",
    "print(len(test_features))\n",
    "\n",
    "#  frequency distribution of the NERC labels\n",
    "\n",
    "print(\"\\nFrequency distribution in train:\")\n",
    "train_count = Counter(training_gold_labels)\n",
    "# sort to facilitate compariso\n",
    "train_count = dict( sorted(train_count.items(), key=lambda x: x[0].lower()) )\n",
    "print(train_count.keys())\n",
    "print(train_count.values())\n",
    "# add proportion for comparison\n",
    "list_train = list(train_count.values())\n",
    "total_train = sum(list_train)\n",
    "print([round(x / total_train, 3) for x in list_train])\n",
    "\n",
    "print(\"Frequency distribution in test:\")\n",
    "test_count = Counter(test_gold_labels)\n",
    "# sort to facilitate comparison\n",
    "test_count = dict( sorted(test_count.items(), key=lambda x: x[0].lower()) )\n",
    "print(test_count.keys())\n",
    "print(test_count.values())\n",
    "# add proportion for comparison\n",
    "list_test = list(test_count.values())\n",
    "total_test = sum(list_test)\n",
    "print([round(x / total_test, 3) for x in list_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discussion 1b\n",
    "Within the training data there are 203621 instances. The frequency in the training data shows that the majority of labels are not named entities (O). The least common label is B-PER and B-ORG, which is a person and an organization respectively. Thus, in the training data we have a very uneven frequency distribution across the different labels. The test data similarly to the training data is also a very uneven label frequency distribution within the data. The most common label is O, which is not a named entity which is 0.825 that is over 100 times more frequent than the least common label. The least common label are again the B-PER and B-ORG like the training data.\n",
    "\n",
    "As can be seen in the previous results (see proportions), the distributions in the training and test data are relatively similar. There are some small differences, e.g. the number of B-MISC is slightly higher in the case of the train data as compared to the test data, whereas B-ORG is more common in the test data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "features_combined = training_features + test_features\n",
    "train_and_test = vec.fit_transform(features_combined)#.toarray()\n",
    "train_input = train_and_test[:len(training_features)]\n",
    "test_input = train_and_test[len(training_features):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LinearSVC()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf.fit(train_input, training_gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.78      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.72       702\n",
      "       B-ORG       0.79      0.52      0.63      1661\n",
      "       B-PER       0.86      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.57      0.59      0.58       216\n",
      "       I-ORG       0.70      0.47      0.56       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.98      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.72      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = lin_clf.predict(test_input)\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(test_gold_labels, test_pred)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discussion 1d\n",
    "* The classifier performs well on the labels O and B-LOC. This is because the labels O and B-LOC are the most common labels in the data. Thus, the classifier is able to predict these labels well. We can see that the F1-score for the 0 label is 0.98 and for the B-LOC it is .72. It is best to look at the F1-score due to the class imbalances we have in the data as seen in the previous results of the frequency distribution.\n",
    "* The classifier performs poorly on the labels I-PER and I-ORG. The poor result of I-PER is relatively unexpected because one would expect that the two least represented data classes I-MISC and I-LOC would be the ones that are predicted poorly. I-PER has a very high recall so it is being overpredicted in comparison with the training labels. It is still one of the least represented class labels and so that could be why the f1-score is low, and maybe the two least represented classes I-MISC and I-LOC are rarely predicted in comparison to I-PER as seen with the lower recall scores."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/bella/PycharmProjects/TextMining/ba-text-mining/lab_sessions/lab2/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "# adapt the path to point to the local copy of the nerc_datasets folder\n",
    "train = ConllCorpusReader('/Users/bella/PycharmProjects/TextMining/ba-text-mining/lab_sessions/lab4/nerc_datasets/CONLL2003',\n",
    "                          'train.txt', # this will load the file 'train.txt', for the exercise you also need to load 'test.xt'\n",
    "                          ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "input_vectors=[]\n",
    "labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "\n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        input_vectors.append(vector)\n",
    "        labels.append(ne_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearSVC()",
      "text/html": "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf2 = svm.LinearSVC()\n",
    "lin_clf2.fit(input_vectors, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train = ConllCorpusReader('/Users/bella/PycharmProjects/TextMining/ba-text-mining/lab_sessions/lab4/nerc_datasets/CONLL2003',\n",
    "                          'test.txt', # this will load the file 'train.txt', for the exercise you also need to load 'test.xt'\n",
    "                          ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_input_vectors=[]\n",
    "test_labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "\n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        test_input_vectors.append(vector)\n",
    "        test_labels.append(ne_label)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.80      0.78      1668\n",
      "      B-MISC       0.72      0.70      0.71       702\n",
      "       B-ORG       0.69      0.64      0.66      1661\n",
      "       B-PER       0.75      0.67      0.71      1617\n",
      "       I-LOC       0.51      0.42      0.46       257\n",
      "      I-MISC       0.60      0.54      0.57       216\n",
      "       I-ORG       0.48      0.33      0.39       835\n",
      "       I-PER       0.59      0.50      0.54      1156\n",
      "           O       0.97      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.68      0.62      0.64     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred2 = lin_clf2.predict(test_input_vectors)\n",
    "report2 = classification_report(test_labels, test_pred2)\n",
    "print(report2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discussion 1e\n",
    "The results of the classifier we built in 1d vs. 1e are relatively similar. In 1e we used vector word embeddings. In 1e we see that overall that the classifier in 1d performed marginally better although the weighted averages of the f1-scores for both classifiers were equal to 0.92. The accuracy in the embeddings performed marginall ybetter with a score of 0.93 compared to 1d classifier's 0.92. However, the macro-average for precision and recall in 1e were lower than 1d and the weighted average of precision was also lower in 1e than 1d. The weighted average wsa 0.01 better in 1e than 1d. This is because the classifier in 1e has significantly better recall scores in labels such as B-MISC with 0.70 instead of 0.66 and B-ORG with a score of 0.64 instead of 0.52. The decrease in precision averages in 1e in comparison tp 1d is most likely due to the fcat that the classifier in 1e under-predicted I-ORG and resulted in a precision score for that label of 0.48 in comparison to the 0.70 score from the classifier in 1d.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9y/07y676w93f97kg8_g01rshw00000gn/T/ipykernel_98496/2866609076.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)\n",
      "Skipping line 281837: expected 25 fields, saw 34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = '/Users/bella/PycharmProjects/TextMining/ba-text-mining/lab_sessions/lab4/nerc_datasets/kaggle/ner_v2.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1050795"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'O'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_dataset[\"word\"][2]\n",
    "kaggle_dataset[\"pos\"][2]\n",
    "kaggle_dataset[\"tag\"][2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "#train = ConllCorpusReader('C:\\\\Users\\\\Gebruiker\\\\ba-text-mining-1\\\\lab_sessions\\\\lab4\\\\CONLL2003\\\\CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    token = df_train[\"word\"][i]\n",
    "    pos = df_train[\"pos\"][i]\n",
    "    tag = df_train[\"tag\"][i]\n",
    "    a_dict = {\n",
    "       # add features\n",
    "        'word': token,\n",
    "        'pos': pos\n",
    "    }\n",
    "    training_features.append(a_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "           id    lemma next-lemma next-next-lemma next-next-pos  \\\n0      100000        \"      death              to            TO   \n1      100001    death         to         america           NNP   \n2      100002       to    america               \"            ``   \n3      100003  america          \"           march           VBD   \n4      100004        \"      march         through            IN   \n...       ...      ...        ...             ...           ...   \n19995  119995   reject          a          packag            NN   \n19996  119996        a     packag              of            IN   \n19997  119997   packag         of          measur           NNS   \n19998  119998       of     measur          includ           VBG   \n19999  119999   measur     includ             one            CD   \n\n      next-next-shape next-next-word next-pos   next-shape  next-word  ...  \\\n0           lowercase             to       NN  capitalized      Death  ...   \n1         capitalized        America       TO    lowercase         to  ...   \n2               punct              \"      NNP  capitalized    America  ...   \n3           lowercase        marched       ``        punct          \"  ...   \n4           lowercase        through      VBD    lowercase    marched  ...   \n...               ...            ...      ...          ...        ...  ...   \n19995       lowercase        package       DT    lowercase          a  ...   \n19996       lowercase             of       NN    lowercase    package  ...   \n19997       lowercase       measures       IN    lowercase         of  ...   \n19998       lowercase      including      NNS    lowercase   measures  ...   \n19999       lowercase            one      VBG    lowercase  including  ...   \n\n      prev-prev-lemma prev-prev-pos prev-prev-shape prev-prev-word  \\\n0            demonstr           NNS     capitalized  Demonstrators   \n1               chant           VBG       lowercase       chanting   \n2                   \"            ``           punct              \"   \n3               death            NN     capitalized          Death   \n4                  to            TO       lowercase             to   \n...               ...           ...             ...            ...   \n19995           voter           NNS       lowercase         voters   \n19996        narrowli            RB       lowercase       narrowly   \n19997          reject           VBD       lowercase       rejected   \n19998               a            DT       lowercase              a   \n19999          packag            NN       lowercase        package   \n\n        prev-shape prev-word sentence_idx        shape      word    tag  \n0        lowercase  chanting       4544.0        punct         \"      O  \n1            punct         \"       4544.0  capitalized     Death      O  \n2      capitalized     Death       4544.0    lowercase        to      O  \n3        lowercase        to       4544.0  capitalized   America  B-geo  \n4      capitalized   America       4544.0        punct         \"  I-geo  \n...            ...       ...          ...          ...       ...    ...  \n19995    lowercase  narrowly       5469.0    lowercase  rejected      O  \n19996    lowercase  rejected       5469.0    lowercase         a      O  \n19997    lowercase         a       5469.0    lowercase   package      O  \n19998    lowercase   package       5469.0    lowercase        of      O  \n19999    lowercase        of       5469.0    lowercase  measures      O  \n\n[20000 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>lemma</th>\n      <th>next-lemma</th>\n      <th>next-next-lemma</th>\n      <th>next-next-pos</th>\n      <th>next-next-shape</th>\n      <th>next-next-word</th>\n      <th>next-pos</th>\n      <th>next-shape</th>\n      <th>next-word</th>\n      <th>...</th>\n      <th>prev-prev-lemma</th>\n      <th>prev-prev-pos</th>\n      <th>prev-prev-shape</th>\n      <th>prev-prev-word</th>\n      <th>prev-shape</th>\n      <th>prev-word</th>\n      <th>sentence_idx</th>\n      <th>shape</th>\n      <th>word</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100000</td>\n      <td>\"</td>\n      <td>death</td>\n      <td>to</td>\n      <td>TO</td>\n      <td>lowercase</td>\n      <td>to</td>\n      <td>NN</td>\n      <td>capitalized</td>\n      <td>Death</td>\n      <td>...</td>\n      <td>demonstr</td>\n      <td>NNS</td>\n      <td>capitalized</td>\n      <td>Demonstrators</td>\n      <td>lowercase</td>\n      <td>chanting</td>\n      <td>4544.0</td>\n      <td>punct</td>\n      <td>\"</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100001</td>\n      <td>death</td>\n      <td>to</td>\n      <td>america</td>\n      <td>NNP</td>\n      <td>capitalized</td>\n      <td>America</td>\n      <td>TO</td>\n      <td>lowercase</td>\n      <td>to</td>\n      <td>...</td>\n      <td>chant</td>\n      <td>VBG</td>\n      <td>lowercase</td>\n      <td>chanting</td>\n      <td>punct</td>\n      <td>\"</td>\n      <td>4544.0</td>\n      <td>capitalized</td>\n      <td>Death</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100002</td>\n      <td>to</td>\n      <td>america</td>\n      <td>\"</td>\n      <td>``</td>\n      <td>punct</td>\n      <td>\"</td>\n      <td>NNP</td>\n      <td>capitalized</td>\n      <td>America</td>\n      <td>...</td>\n      <td>\"</td>\n      <td>``</td>\n      <td>punct</td>\n      <td>\"</td>\n      <td>capitalized</td>\n      <td>Death</td>\n      <td>4544.0</td>\n      <td>lowercase</td>\n      <td>to</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100003</td>\n      <td>america</td>\n      <td>\"</td>\n      <td>march</td>\n      <td>VBD</td>\n      <td>lowercase</td>\n      <td>marched</td>\n      <td>``</td>\n      <td>punct</td>\n      <td>\"</td>\n      <td>...</td>\n      <td>death</td>\n      <td>NN</td>\n      <td>capitalized</td>\n      <td>Death</td>\n      <td>lowercase</td>\n      <td>to</td>\n      <td>4544.0</td>\n      <td>capitalized</td>\n      <td>America</td>\n      <td>B-geo</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100004</td>\n      <td>\"</td>\n      <td>march</td>\n      <td>through</td>\n      <td>IN</td>\n      <td>lowercase</td>\n      <td>through</td>\n      <td>VBD</td>\n      <td>lowercase</td>\n      <td>marched</td>\n      <td>...</td>\n      <td>to</td>\n      <td>TO</td>\n      <td>lowercase</td>\n      <td>to</td>\n      <td>capitalized</td>\n      <td>America</td>\n      <td>4544.0</td>\n      <td>punct</td>\n      <td>\"</td>\n      <td>I-geo</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>119995</td>\n      <td>reject</td>\n      <td>a</td>\n      <td>packag</td>\n      <td>NN</td>\n      <td>lowercase</td>\n      <td>package</td>\n      <td>DT</td>\n      <td>lowercase</td>\n      <td>a</td>\n      <td>...</td>\n      <td>voter</td>\n      <td>NNS</td>\n      <td>lowercase</td>\n      <td>voters</td>\n      <td>lowercase</td>\n      <td>narrowly</td>\n      <td>5469.0</td>\n      <td>lowercase</td>\n      <td>rejected</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>119996</td>\n      <td>a</td>\n      <td>packag</td>\n      <td>of</td>\n      <td>IN</td>\n      <td>lowercase</td>\n      <td>of</td>\n      <td>NN</td>\n      <td>lowercase</td>\n      <td>package</td>\n      <td>...</td>\n      <td>narrowli</td>\n      <td>RB</td>\n      <td>lowercase</td>\n      <td>narrowly</td>\n      <td>lowercase</td>\n      <td>rejected</td>\n      <td>5469.0</td>\n      <td>lowercase</td>\n      <td>a</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>119997</td>\n      <td>packag</td>\n      <td>of</td>\n      <td>measur</td>\n      <td>NNS</td>\n      <td>lowercase</td>\n      <td>measures</td>\n      <td>IN</td>\n      <td>lowercase</td>\n      <td>of</td>\n      <td>...</td>\n      <td>reject</td>\n      <td>VBD</td>\n      <td>lowercase</td>\n      <td>rejected</td>\n      <td>lowercase</td>\n      <td>a</td>\n      <td>5469.0</td>\n      <td>lowercase</td>\n      <td>package</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>119998</td>\n      <td>of</td>\n      <td>measur</td>\n      <td>includ</td>\n      <td>VBG</td>\n      <td>lowercase</td>\n      <td>including</td>\n      <td>NNS</td>\n      <td>lowercase</td>\n      <td>measures</td>\n      <td>...</td>\n      <td>a</td>\n      <td>DT</td>\n      <td>lowercase</td>\n      <td>a</td>\n      <td>lowercase</td>\n      <td>package</td>\n      <td>5469.0</td>\n      <td>lowercase</td>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>119999</td>\n      <td>measur</td>\n      <td>includ</td>\n      <td>one</td>\n      <td>CD</td>\n      <td>lowercase</td>\n      <td>one</td>\n      <td>VBG</td>\n      <td>lowercase</td>\n      <td>including</td>\n      <td>...</td>\n      <td>packag</td>\n      <td>NN</td>\n      <td>lowercase</td>\n      <td>package</td>\n      <td>lowercase</td>\n      <td>of</td>\n      <td>5469.0</td>\n      <td>lowercase</td>\n      <td>measures</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>20000 rows × 25 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.reset_index(inplace=True, drop=True)\n",
    "df_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "#train = ConllCorpusReader('C:\\\\Users\\\\Gebruiker\\\\ba-text-mining-1\\\\lab_sessions\\\\lab4\\\\CONLL2003\\\\CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "    token = df_test[\"word\"][i]\n",
    "    pos = df_test[\"pos\"][i]\n",
    "    tag = df_test[\"tag\"][i]\n",
    "    a_dict = {\n",
    "       # add features\n",
    "        'word': token,\n",
    "        'pos': pos\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(tag)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances in train:\n",
      "100000\n",
      "Instances in test:\n",
      "20000\n",
      "\n",
      "Frequency distribution in train:\n",
      "dict_keys([])\n",
      "dict_values([])\n",
      "[]\n",
      "Frequency distribution in test:\n",
      "dict_keys(['B-art', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per', 'B-tim', 'I-geo', 'I-gpe', 'I-nat', 'I-org', 'I-per', 'I-tim', 'O'])\n",
      "dict_values([4, 741, 296, 8, 397, 333, 393, 156, 2, 4, 321, 319, 108, 16918])\n",
      "[0.0, 0.037, 0.015, 0.0, 0.02, 0.017, 0.02, 0.008, 0.0, 0.0, 0.016, 0.016, 0.005, 0.846]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# number of instances\n",
    "print(\"Instances in train:\")\n",
    "print(len(training_features))\n",
    "print(\"Instances in test:\")\n",
    "print(len(test_features))\n",
    "\n",
    "#  frequency distribution of the NERC labels\n",
    "\n",
    "print(\"\\nFrequency distribution in train:\")\n",
    "train_count = Counter(training_gold_labels)\n",
    "# sort to facilitate compariso\n",
    "train_count = dict( sorted(train_count.items(), key=lambda x: x[0].lower()) )\n",
    "print(train_count.keys())\n",
    "print(train_count.values())\n",
    "# add proportion for comparison\n",
    "list_train = list(train_count.values())\n",
    "total_train = sum(list_train)\n",
    "print([round(x / total_train, 3) for x in list_train])\n",
    "\n",
    "print(\"Frequency distribution in test:\")\n",
    "test_count = Counter(test_gold_labels)\n",
    "# sort to facilitate comparison\n",
    "test_count = dict( sorted(test_count.items(), key=lambda x: x[0].lower()) )\n",
    "print(test_count.keys())\n",
    "print(test_count.values())\n",
    "# add proportion for comparison\n",
    "list_test = list(test_count.values())\n",
    "total_test = sum(list_test)\n",
    "print([round(x / total_test, 3) for x in list_test])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "features_combined = training_features + test_features\n",
    "train_and_test = vec.fit_transform(features_combined)  #.toarray()\n",
    "train_input = train_and_test[:len(training_features)]\n",
    "test_input = train_and_test[len(training_features):]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [100000, 0]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m svm\n\u001B[1;32m      2\u001B[0m lin_clf \u001B[38;5;241m=\u001B[39m svm\u001B[38;5;241m.\u001B[39mLinearSVC()\n\u001B[0;32m----> 3\u001B[0m \u001B[43mlin_clf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_gold_labels\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/sklearn/svm/_classes.py:263\u001B[0m, in \u001B[0;36mLinearSVC.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit the model according to the given training data.\u001B[39;00m\n\u001B[1;32m    239\u001B[0m \n\u001B[1;32m    240\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;124;03m    An instance of the estimator.\u001B[39;00m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m--> 263\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    266\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    271\u001B[0m check_classification_targets(y)\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_ \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(y)\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/sklearn/base.py:565\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    563\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[1;32m    564\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 565\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    566\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/sklearn/utils/validation.py:1124\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m   1106\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[1;32m   1107\u001B[0m     X,\n\u001B[1;32m   1108\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1119\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1120\u001B[0m )\n\u001B[1;32m   1122\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[0;32m-> 1124\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[0;32m~/TextMining/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[0;34m(*arrays)\u001B[0m\n\u001B[1;32m    395\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[1;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 397\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    399\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[1;32m    400\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [100000, 0]"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(train_input, training_gold_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
